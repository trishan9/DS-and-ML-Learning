# DS-and-ML-Learning
___

This repo consists of my whole Data Science and Machine Learning journey and here I will be documenting my complete journey! Inspired from: [**iamshishirbhattarai/100DaysOfMachineLearning**](https://github.com/iamshishirbhattarai/100DaysOfMachineLearning)
___
## Syllabus
This is just a pre-setup and things are added as exploration continues !!

| **S.N.** | **Books and Lessons (Resources)**                                                                                                 | **Status** |
|----------|-----------------------------------------------------------------------------------------------------------------------------------|------------|
| **1.**   | [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction)          | ⏳          |
| **2.**   | [**Machine Learning Scientist With Python**](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python) | ⏳          |
| **3.**   | [**Associate Data Scientist in Python**](https://app.datacamp.com/learn/career-tracks/associate-data-scientist-in-python) | ⏳          |
| **4.**   | [**Machine Learning A-Z: AI, Python & R**](https://www.udemy.com/course/machinelearning/) | ⏳          |

___

## Projects Completed

| **S.N.** | **Project Title** | **Status** |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| 1.       | [**Nobel Prize Winner Data Analysis**](https://github.com/trishan9/DS-and-ML-Projects/blob/main/Nobel-Prize-Winner-Data-Analysis/notebook.ipynb)  | ✅         |
| 2.       | [**VC Profit Predictor**](https://github.com/trishan9/VC-Profit-Predictor)  | ✅         |
| 3.       | [**Breast Cancer Prediction**](https://github.com/trishan9/DS-and-ML-Projects/blob/main/Breast-Cancer-Prediction/breast_cancer_prediction.ipynb)  |✅          |
| 4.       | [**Tweets Sentiment Analysis**](https://github.com/trishan9/DS-and-ML-Projects/blob/main/Tweets-Sentiment-Analysis/sentiment-analysis.ipynb)  |✅          |
| 5.       | [**Brain Tumor Detection**](https://github.com/trishan9/DS-and-ML-Projects/blob/main/Brain-Tumor-Prediction/brain_tumor_prediction.ipynb)  |✅          |
| 6.       | [**Student Performance Evaluation**](https://github.com/trishan9/DS-and-ML-Projects/blob/main/Student-Performance-Evaluation/student_performance_evaluation.ipynb)  |✅          |


## Topics Learnt

| **Days**        | **Learnt Topics**                                                                                                                                                            | **Resources used**                                                                                                                                                                                                                                  |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [01](#day-1)   |         Linear Regression, Cost Function and Optimization                                                                                                       |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction)                                                                                                                                |
| [02](#day-2)   |         Gradient Descent Algorithm, Intuition, Implementation                                                                                                       |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction)                                                                                                                                |
| [03](#day-3)   |         Multiple Feature Linear Regression                                                                                                       |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction)                                                                                                                                |
| [04](#day-4)   |         ML Process and Data Pre-processing                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [05](#day-5)   |         Data Pre-processing Techniques in R                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [06](#day-6)   |         Vectorization, Gradient Descent for Multiple Linear Regression                                                                                                       |          [**Machine Learning Specialization**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [07](#day-7)   |         Feature Scaling & Gradient Descent Optimization                                                                                                       |          [**Machine Learning Specialization**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [08](#day-8)   |         Exploratory Data Analysis in Python                                                                                                       |          [**Associate Data Scientist in Python**](https://app.datacamp.com/learn/career-tracks/associate-data-scientist-in-python)                                                                                                                                |
| [09](#day-9)   |         Simple Linear Regression in Python & R                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [10](#day-10)   |         Multiple Linear Regression in Python                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [11](#day-11)   |         Advanced Techniques in Multiple Linear Regression                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [12](#day-12)   |         Polynomial Regression                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/), [**CampusX**](https://youtu.be/BNWLf3cKdbQ?si=SC-EgkUVHpW2k-Zi)                                                                                                                                |
| [13](#day-13)   |         Bias-Variance Trade-off                                                                                                       |          **StatQuest, CampusX, Krish Naik**                                                                                                                                |
| [14](#day-14)   |         Ridge & Lasso Regression                                                                                                       |          **StatQuest, CampusX, Krish Naik**                                                                                                                                |
| [15](#day-15)   |         Ridge & Lasso Regression Implementation in Python                                                                                                       |          **YouTube**                                                                                                                                |
| [16](#day-16)   |         ElasticNet Regression & SVR Deepdive                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/), [**Paper1**](https://bmcproc.biomedcentral.com/articles/10.1186/1753-6561-6-S2-S10), [**Paper2**](https://core.ac.uk/download/pdf/81523322.pdf)                                                                                                                                 |
| [17](#day-17)   |         Decision Tree Regression                                                                                                       |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [18](#day-18)   |         Implementing Decision Tree Regression in Python                                                                                                    |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [19](#day-19)   |         Random Forest Regression & All Regression Model Comparison                                                                                                   |          [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                                                                                                                |
| [20](#day-20)   |         Understanding Logistic Regression - Binary Classification                                                                                                  |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction), [**MLU Explain**](https://mlu-explain.github.io/logistic-regression/)                                                                                                                           |
| [21](#day-21)   |         Cost Function and Gradient Descent in Logistic Regression                                                                                                  |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction)                                                                                                                         |
| [22](#day-22)   |         Deepdive and Implementation of Logistic Regression                                                                                                  |          [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction), [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                 |
| [23](#day-23)   |         KNN, Cross-Validation, Hyperparameter Tuning, Model Evaluation Techniques, Pipelining                                                                                                  |          [**Machine Learning Scientist With Python**](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python), [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/)                                 |
| [24](#day-24)   |         Support Vector Machines                                                                                                  |     [**Machine Learning A-Z**](https://www.udemy.com/course/machinelearning/), [**Blog 1**](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47), [**Blog 2**](https://medium.com/low-code-for-advanced-data-science/support-vector-machines-svm-an-intuitive-explanation-b084d6238106), **YouTube (StatQuest, Krish Naik, Visually Explained)**            |
| [25](#day-25)   |         Implementation of Support Vector Machines                                                                                                 |     [**Kaggle**](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)      |
| [26](#day-26)   |         Implementation of Image Classification for Brain Tumor Detection                                                                                               |     [**Kaggle Dataset**](https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor/)      |
| [27](#day-27)   |         Implementation of Classification Algorithms for Student Performance Evaluation                                                                                               |     [**Kaggle Dataset**](https://www.kaggle.com/datasets/rabieelkharoua/students-performance-dataset/)      |
| [28](#day-28)   |         Decision Trees, Random Forest, and Extra Trees Classification Algorithms                                                                                               |     [**DataCamp Tutorial 1**](https://www.datacamp.com/tutorial/decision-tree-classification-python), [**DataCamp Tutorial 2**](https://www.datacamp.com/tutorial/random-forests-classifier-python), **YouTube**      |



___
## Day 1
### Topic: Linear Regression Fundamentals
*Date: October 3, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Linear Regression
- Mathematical foundations
- Cost function and optimization
- Model parameters and fitting

### Detailed Notes 📝

**Linear Regression Model Basics**

- Linear regression is a supervised learning algorithm for predicting a continuous output variable (y) based on input features (x)
- The model represents a linear relationship: `f(x) = wx + b`
  - w: weight/slope
  - b: bias/y-intercept
- Use case: Predicting numerical values (e.g., house prices, stock prices, temperature forecasting)

**Model Components**

![image](https://github.com/user-attachments/assets/a874b11c-c6ed-44a6-8956-bb1127aa8ab9)

- Training set: Collection of input-output pairs for learning
- Features (x): Input variables
- Targets (y): Actual output values
- Predictions (ŷ): Model's estimated outputs
- Model function: f(x) = wx + b

**Mathematical Framework & Cost Function**

![image](https://github.com/user-attachments/assets/9d6822c0-0866-46b7-8d09-519d6eec4863)

- Model equation: **f<sub>w,b</sub>(x) = wx + b**
- Parameters:
  - w (weight): Determines slope
  - b (bias): Determines y-intercept
- Simplified version: **f<sub>w</sub>(x) = wx (when b = 0)** (just for learning, as it can be visualized in 2D easily)
- Cost Function: Measures how well our model fits the data (quantifies the error between predictions and actual results.)
- Squared Error Cost Function: **J(w,b) = (1/2m) ∑(f<sub>w,b</sub>(x⁽ⁱ⁾) - y⁽ⁱ⁾)²** where,
  - m: number of training examples
  - x⁽ⁱ⁾: i-th input feature
  - y⁽ⁱ⁾: i-th actual output

**Optimization Goal (Minimizing the Cost Function)**

![image](https://github.com/user-attachments/assets/5de4831b-78a2-4a33-954c-a808c5a29bf7)

- Objective: Find best values of w and b that minimize J(w,b) which tells us how well our linear model fits the data.
- The optimal point occurs where:
  - Cost function J(w) reaches its minimum
  - In the example graph, w ≈ 1 gives minimum cost

**Visual Intuition**

![image](https://github.com/user-attachments/assets/6fd68e1f-a34f-4a89-83d9-ad912aff7fba)

- Cost function forms a soup bowl-shaped surface in 3D
- Global minimum exists at the bottom of the bowl
- Goal is to find the coordinates (w,b) at this minimum

**Key Takeaways 🔑**

- Linear regression finds a linear relationship between input and output
- Model is represented by f(x) = wx + b
- Cost function measures prediction errors
- Goal is to minimize cost function by finding optimal w and b
- Visualization helps understand the optimization landscape

___
## Day 2
### Topic: Gradient Descent for Linear Regression
*Date: October 4, 2024*

### Today's Learning Objectives Completed ✅
- Understanding Gradient Descent algorithm
- Learning rate and convergence concepts
- Implementation in Python
- Making predictions with optimized parameters

### Detailed Notes 📝

#### Gradient Descent Algorithm
![swappy-20241004_085011](https://github.com/user-attachments/assets/9cc47add-4ec3-4944-b3cc-b8d908902a0e)


The algorithm:
Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent as defined by the negative of the gradient. It is widely used in machine learning, especially for minimizing cost functions in algorithms like linear regression and logistic regression.

In machine learning, the goal is often to minimize a cost function to get the optimal set of model parameters (like `w, b` in linear regression). Gradient Descent finds these optimal parameters by iteratively updating them in the direction where the cost decreases most rapidly.
- Initialize the parameters (w and b) to some random values.
- Calculate the cost function for the current parameters.
- Compute the gradient of the cost function with respect to the parameters.
- Simultaneously update the parameters using the gradient to reduce the cost function.
- Repeat this process until the cost function converges (i.e., no significant change in cost).
- Formula:
  ```
  w = w - α * (∂/∂w)J(w,b)
  b = b - α * (∂/∂b)J(w,b)
  ```
  - α (alpha): Learning rate (hyperparameter that controls the size of the steps we take in the direction of the gradient)
  - ∂/∂w, ∂/∂b: Partial derivatives of cost function (controls the direction : where to take step, either left or right)


#### Gradient Descent Intuition
![swappy-20241004_085059](https://github.com/user-attachments/assets/c414b2a7-fe2c-4226-9df0-c6bbe7587852)


- When slope is positive (>0):
  - w decreases (moves left)
  - w = w - α * (positive number)
- When slope is negative (<0):
  - w increases (moves right)
  - w = w - α * (negative number)
- Algorithm naturally moves toward minimum

#### Learning Rate (α) Considerations
![swappy-20241004_085233](https://github.com/user-attachments/assets/55ae693d-19d5-4a31-97b3-b29d3c384162)


Critical aspects:
- If α is too small:
  - Gradient descent will be slow
  - Takes many iterations to converge
- If α is too large:
  - May overshoot the minimum
  - Might fail to converge or even diverge
- Need to choose appropriate learning rate

#### Partial Derivatives (Mathematical Detail)
![swappy-20241004_085357](https://github.com/user-attachments/assets/50348dc6-dff0-4de6-812d-3c1d2e55964f)


Derivatives for batch gradient descent:
```
∂/∂w J(w,b) = (1/m) ∑(fw,b(x⁽ⁱ⁾) - y⁽ⁱ⁾)x⁽ⁱ⁾
∂/∂b J(w,b) = (1/m) ∑(fw,b(x⁽ⁱ⁾) - y⁽ⁱ⁾)
```

#### Implementation Results
![swappy-20241004_221626](https://github.com/user-attachments/assets/22041367-a0a1-40fb-b019-a27e3b75b947)


Successfully implemented gradient descent:
- Cost function converged after ~10000 iterations (in plot only first 100 cost history is visualized)
- Final parameters: w ≈ 200, b ≈ 100
- Sample predictions:
  - 1000 sqft house: $300,000
  - 1200 sqft house: $340,000
  - 2000 sqft house: $500,000

#### Key Takeaways 🔑
1. Gradient descent is an iterative optimization algorithm
2. Learning rate is crucial for successful convergence
3. Must update parameters simultaneously
4. Batch gradient descent uses all training examples in comparsion to Stochastic & Mini-Batch
5. Visualization of cost function helps track convergence

___
## Day 3
### Topic: Multiple Linear Regression
*Date: October 5, 2024*

### Today's Learning Objectives Completed ✅
- Understanding Multiple Feature Linear Regression
- Vector notation in Linear Regression
- Feature representation and indexing
- Extended model equations

### Detailed Notes 📝

#### Multiple Features Introduction
![swappy-20241005_214718](https://github.com/user-attachments/assets/ae8f424d-5ad9-420a-aef7-e6a1bab277cf)


Important notation:
- n = number of features
- m = number of training examples
- x<sup>(i)</sup> = features of i<sup>th</sup> training example
- x<sub>j</sub><sup>(i)</sup> = value of feature j in i<sup>th</sup> training example

Example from the data:
- x<sup>(2)</sup> = [1416  3  2  40] (complete 2nd training example)
- x<sub>3</sub><sup>(2)</sup> = 2 (3rd feature of 2nd training example)

#### Model Extension
![swappy-20241005_214726](https://github.com/user-attachments/assets/8e3e9689-8c16-49a4-8e0d-ecd69540afb3)


Evolution of the model:
- Previously: f<sub>w,b</sub>(x) = wx + b
- Now with multiple features:
  ```
  fw,b(x) = w₁x₁ + w₂x₂ + w₃x₃ + w₄x₄ + b
  ```

Example house price prediction:
```
fw,b(x) = 0.1x₁ + 4x₂ + 10x₃ - 2x₄ + 80
```
where:
- x₁: size in feet²
- x₂: number of bedrooms
- x₃: number of floors
- x₄: age of home in years
- b = 80: base price

#### Vector Notation
![swappy-20241005_214740](https://github.com/user-attachments/assets/eadb2826-0e00-4101-8f95-b40e344b7966)


Modern representation using vectors:
- w⃗ = [w₁ w₂ w₃ ... wₙ] (parameter vector)
- x⃗ = [x₁ x₂ x₃ ... xₙ] (feature vector)
- b is a single number (scalar)

Final model equation using dot product:
```
fw,b(x) = w⃗ · x⃗ + b = w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ + b
```

**Important Note**: This is multiple linear regression, not multivariate regression. The distinction is that we have multiple features (variables) but still predict a single output value.

#### Key Takeaways 🔑
1. Multiple features allow more complex and accurate predictions
2. Vector notation simplifies representation of multiple features
3. Dot product provides elegant mathematical formulation
4. Each feature has its own weight parameter (w)
5. Base price (b) remains a single scalar value

#### Practical Implementation Tips 💡
- Use vectors and matrices for efficient computation
- Keep track of feature indices carefully
- Document feature meanings and units
- Consider feature scaling for better performance
- Use proper indexing notation in code

___
## Day 4
### Topic: Machine Learning Process and Data Pre-processing
*Date: October 6, 2024*

### Today's Learning Objectives Completed ✅
- Understanding the machine learning process
- Data Pre-processing theory
- Data Pre-processing implementation in Python using Scikit-learn
- Concepts of encoding and handling missing data
- Splitting dataset into training and test sets
- Feature scaling: understanding types and necessity
- Implementation of data pre-processing in Python

### Detailed Notes 📝

#### The Machine Learning Process Overview
![image](https://github.com/user-attachments/assets/aa31449a-9941-42e7-a683-8459d7d8f9bc)

The machine learning process can be broken down into three main stages:
1. **Data Pre-Processing**:
    - Import the data
    - Clean the data (handle missing values, encoding categorical data)
    - Split into training and test sets
    - Feature scaling (normalization/standardization)

2. **Modelling**:
    - Build the model
    - Train the model
    - Make predictions

3. **Evaluation**:
    - Calculate performance metrics
    - Make a verdict

#### Data Pre-processing Steps

1. **Importing the Data**:
    This involves loading the dataset into your Python environment. In my case, I used the Pandas library to import CSV data into a DataFrame.

2. **Handling Missing Data**:
    Missing data can be handled in multiple ways:
    - Removing the missing data rows (not recommended in all cases).
    - Replacing the missing values with mean, median, or most frequent values. I replaced missing data in this case using `SimpleImputer` from Scikit-learn.

3. **Encoding Categorical Data**:
    Categorical variables must be encoded as numerical values for ML algorithms. I used `LabelEncoder` for encoding categorical variables.

4. **Splitting the Dataset**:
    The dataset is split into a **Training set** (to train the model) and a **Test set** (to evaluate model performance).

    ![image](https://github.com/user-attachments/assets/d490577f-72bd-45d1-9d6f-c82b81f42688)


    - I used the `train_test_split` function from Scikit-learn to split the data in an 80:20 ratio for training and testing.

5. **Feature Scaling**:
    Feature scaling ensures that all the features are on the same scale, improving the performance of machine learning models.

   ![image](https://github.com/user-attachments/assets/930c2fa0-804b-49e9-b89f-5c5d558b76d9)


    There are two types of feature scaling:
    - **Normalization**: Scales values between 0 and 1.
    - **Standardization**: Scales data with a mean of 0 and standard deviation of 1.

    ![image](https://github.com/user-attachments/assets/bf5505f9-de35-45fb-9105-1f57c89aaef5)


#### Python Implementation 🖥️
I implemented Data Pre-processing in Python using scikit-learn:

![image](https://github.com/user-attachments/assets/b603e2bf-b2f8-4591-9a1e-3df3529177dd)


#### Key Takeaways 🔑
- Pre-processing is crucial for ensuring that data is clean and well-prepared for training.
- Handling missing data can have a significant impact on model performance.
- Encoding categorical data is necessary to convert text labels into a format that machine learning models can understand.
- Feature scaling ensures that all features contribute equally to the model's learning process.
- Splitting data ensures that model evaluation is performed on unseen data, preventing overfitting.

#### Some diagrams
**ML Process Flow**


![image](https://github.com/user-attachments/assets/3a97d53b-d756-43be-8129-838aee56a028)


**Dataset Splitting and Scaling Process**


![image](https://github.com/user-attachments/assets/fbb325d9-9c00-414d-8c65-a3adc9a9cf70)


**Feature Scaling Methods Comparison**


![image](https://github.com/user-attachments/assets/139cdc5c-dec9-4867-8f28-4db9d4cdebee)


**Data Preprocessing Steps**


![image](https://github.com/user-attachments/assets/55396f9c-4a9d-4da9-bc0f-6af35e406a80)


___
## Day 5
### Topic: Data Pre-processing Techniques in R
*Date: October 7, 2024*

### Today's Learning Objectives Completed ✅
- Deep dive into data preprocessing concepts
- Implemented data preprocessing in R
- Understanding the importance of handling missing data
- Encoding of categorical data in R
- Importance of feature scaling and its correct application
- Concept of avoiding information leakage by scaling after dataset splitting

### Detailed Notes 📝

#### Advanced Data Pre-processing Techniques

Today, I expanded my understanding of data preprocessing and implemented these techniques using R programming. Here are the detailed concepts and methods I focused on:

1. **Handling Missing Data**:
   - Missing data can create bias and reduce model accuracy.
   - Various strategies to handle missing data include removing missing values, replacing them with mean, median, mode, or more sophisticated techniques.
   - Importance: Ensuring that no important information is lost while maintaining the dataset's integrity.

2. **Encoding Categorical Data**:
   - Machine learning algorithms work better with numerical data; hence, categorical variables need to be converted into numbers.
   - Encoding techniques like One-Hot Encoding and Label Encoding were explored using R functions.
   - In R, I used the `factor()` function for Label Encoding.
   - This step is crucial for ensuring that algorithms can interpret categorical features properly.

3. **Splitting the Dataset into Training and Test Sets**:
   - Splitting the dataset ensures that the model is trained on one part of the data and tested on another, which is crucial for evaluating its performance.
   - In R, I used the `sample.split()` function from the `caTools` package to divide the dataset into training and test sets.
   - Proper splitting prevents the model from overfitting on the training data and ensures a fair evaluation on unseen data.

4. **Feature Scaling**:
   - Feature scaling standardizes the range of independent variables or features of the dataset.
   - Two main methods used:
     - **Normalization**: Scales data between 0 and 1.
     - **Standardization**: Scales data to have a mean of 0 and a standard deviation of 1.
   - Scaling in R was performed using functions like `scale()` for Standardization.

5. **Avoiding Information Leakage**:
   - **Information leakage** occurs when data from the test set influences the model training process, leading to over-optimistic performance estimates.
   - To prevent this, feature scaling should be done **after** splitting the dataset into training and test sets.
   - By scaling only the training set first, we ensure that the test set remains completely unseen during model training, preserving its role as unseen data for evaluation.

#### R Implementation 🖥️

Below is my code snippet for data preprocessing using R:

```R
# Importing Dataset
dataset = read.csv('Data.csv')

# Handling missing data
dataset$Age = ifelse(is.na(dataset$Age), ave(dataset$Age, FUN=function(x) mean(x, na.rm=TRUE)), dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary), ave(dataset$Salary, FUN=function(x) mean(x, na.rm=TRUE)), dataset$Salary)

# Encoding Categorical Data
dataset$Country = factor(dataset$Country, levels=c('France', 'Spain', 'Germany'), labels=c(1, 2, 3))
dataset$Purchased = factor(dataset$Purchased, levels=c('Yes', 'No'), labels=c(0, 1))

# Splitting the dataset
library(caTools)
set.seed(100)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Feature Scaling
training_set[, 2:3] = scale(training_set[, 2:3])
test_set[, 2:3] = scale(test_set[, 2:3])
```


![image](https://github.com/user-attachments/assets/343b9296-49e8-4fc4-974e-a4369c113b63)


#### Key Takeaways 🔑
- Proper handling of missing data ensures no important information is lost.
- Encoding categorical variables correctly improves model interpretability.
- Splitting the dataset into training and test sets ensures a fair evaluation of model performance.
- Feature scaling should always be done after splitting the dataset to avoid information leakage.
- Implementing data preprocessing in R is efficient and follows a structured approach using native R functions.
- Following best practices in data preprocessing improves the robustness and reliability of machine learning models.


___
## Day 6
### Topic: Vectorization & Multiple Feature Linear Regression
*Date: October 8, 2024*

**Today's Learning Objectives Completed ✅**
- Vectorization in NumPy for Linear Regression
- Efficient implementation using vector operations
- Gradient Descent for multiple features
- Normal Equation as an alternative approach
- Mathematical notation and implementations

### Detailed Notes 📝

#### Vectorization Fundamentals
Explored how vectorization simplifies the code when implementing learning algorithms. It makes the code not only shorter but also significantly more efficient. By leveraging modern numerical linear algebra libraries (like NumPy) and even GPU hardware, vectorized implementations can run much faster compared to unvectorized versions.

Vectorization involves performing operations on entire arrays or matrices, instead of using explicit loops. It allows us to utilize optimized low-level implementations and take advantage of parallelism.

![image](https://github.com/user-attachments/assets/75e1789d-e2e1-420f-9d17-0d3549279e8c)


**Key Components:**
- Parameters represented as vectors:
  - w = [w₁ w₂ w₃] for weights
  - x = [x₁ x₂ x₃] for features
  - b as a scalar bias term
- Non-vectorized implementation uses loops:
  ```python
  f = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + b
  ```
- Vectorized version uses dot product:
  ```python
  f = np.dot(w,x) + b
  ```

#### Performance Benefits
![image](https://github.com/user-attachments/assets/515d0cf8-f6a4-4790-8322-628da510c800)


**Advantages:**
- Shorter code
- Faster execution
- Parallel computation of element-wise operations
- Efficient memory usage
- Leverages optimized linear algebra libraries
- Scales well with large datasets
- Potential GPU acceleration

**Example of Speed Improvement:**
- Without vectorization: Sequential operations at times t₀, t₁, ..., t₁₅
- With vectorization: Single parallel operation computing all multiplications simultaneously

#### Gradient Descent Implementation
![image](https://github.com/user-attachments/assets/d4fd6482-5e2a-4b51-b97e-5dd3ddb6ef1a)


**Vectorized Updates:**
- Parameters update: w = w - 0.1*d
- Learning rate (α) = 0.1
- Derivatives stored in vector d
- Single operation updates all parameters simultaneously

#### Mathematical Notation
![image](https://github.com/user-attachments/assets/44f3b4ba-dfd8-404b-9ca9-16e3ffcc1c17)


**Improved Notation:**
- Traditional: w₁, w₂, ..., wₙ as separate variables
- Vector notation: w = [w₁ ... wₙ]
- Model function: f(x) = w·x + b
- Simplified gradient descent expressions

#### Multiple Feature Gradient Descent
Studied the mathematical intuition behind gradient descent and how it works for multiple features. Implemented gradient descent using vector operations, which helps in efficiently updating the parameters in each iteration.


![image](https://github.com/user-attachments/assets/a29384f7-670f-47fa-8613-d51a9fa60d48)


**Implementation Details:**
- Handles n ≥ 2 features
- Simultaneous update of all parameters
- Vectorized computation of partial derivatives
- Batch gradient descent with m training examples

#### Normal Equation: An Alternative Approach
 Learned about the normal equation as an alternative approach to solve linear regression problems without using gradient descent. This method directly computes the optimal parameters.


![image](https://github.com/user-attachments/assets/1091bd4e-196a-48b8-93f0-573519f81b48)


**Key Points:**
- Analytical solution specific to linear regression
- Directly solves for optimal w, b without iterations
- One-shot calculation vs. iterative gradient descent

**Advantages:**
- No need to choose learning rate
- No iteration required
- Works well for smaller feature sets

**Disadvantages:**
- Limited to linear regression only
- Computationally expensive for large feature sets (>10,000 features)
- Doesn't generalize to other learning algorithms

**Important Note:**
- While available in many ML libraries, gradient descent remains the recommended approach
- Understanding both methods helps in choosing the right tool for specific scenarios

#### Key Takeaways 🔑
1. Vectorization dramatically improves computational efficiency
2. NumPy's dot product replaces explicit loops
3. Vector operations enable parallel processing
4. Gradient descent scales elegantly with vectorization
5. Modern hardware (especially GPUs) optimized for vector operations
6. Normal equation provides an alternative analytical solution for linear regression


___
## Day 7
### Topic: Feature Scaling & Gradient Descent Optimization
*Date: October 9, 2024*

### Today's Learning Objectives Completed ✅
- Understanding feature scaling techniques
- Learning to check gradient descent convergence
- Mastering learning rate selection
- Exploring feature engineering concepts
- Introduction to polynomial regression

### Detailed Notes 📝

#### Feature Scaling
Feature scaling is crucial when features have very different ranges of values.

**Why Feature Scaling?**
- Helps gradient descent converge faster
- Makes optimization landscape more symmetric
- Prevents features with larger ranges from dominating

**Common Scaling Methods:**
1. **Simple Scaling (Division by Max)**
   - x₁_scaled = x₁/max(x₁)
   - Example: House size (300-2000 sq ft) → (0.15-1.0)

2. **Mean Normalization**
   - x_normalized = (x - μ)/(max - min)
   - Centers data around zero
   - Range typically: [-1, 1]

3. **Z-score Normalization**
   - x_zscore = (x - μ)/σ
   - μ: mean, σ: standard deviation
   - Typically results in range: [-3, 3]

**When to Scale:**
- When features range from -100 to +100
- When features are very small (e.g., 0.001)
- When features are very large (e.g., house prices in thousands)

#### Checking Gradient Descent Convergence
**Key Methods:**
1. **Plot Learning Curve**
   - X-axis: Number of iterations
   - Y-axis: Cost function J
   - Should see consistent decrease

**Signs of Good Convergence:**
- Cost J decreases after every iteration
- Curve eventually flattens out
- No sudden increases in cost

**Convergence Test:**
- Can use epsilon (ε) threshold (e.g., 0.001)
- If J decreases by less than ε, declare convergence
- Visual inspection often more reliable

#### Choosing Learning Rate (α)
**Guidelines:**
1. Start with small values:
   - Try: 0.001, 0.003, 0.01, 0.03, 0.1
   - Increase by ~3x each time

**Warning Signs:**
- Cost function oscillating → α too large
- Cost increasing consistently → α too large or bug in code
- Very slow decrease → α too small

**Debugging Tip:**
- Try very small α
- If cost still doesn't decrease, check code for bugs

#### Feature Engineering
**Creating New Features:**
- Combine existing features meaningfully
- Transform features to capture relationships
- Use domain knowledge to create relevant features

#### Polynomial Regression
**Extending Linear Regression:**
- Fit non-linear relationships
- Add polynomial terms: x², x³
- Can use different transformations:
  - Square root: √x
  - Powers: x², x³
  - Combinations of features

**Important Considerations:**
- Higher-degree polynomials need more feature scaling
- x² ranges from 1 to 1,000,000 if x ranges from 1 to 1,000
- x³ ranges even larger

### Key Takeaways
1. Feature scaling is crucial for efficient gradient descent
2. Learning curves help diagnose convergence issues
3. Choose learning rate through systematic experimentation
4. Feature engineering can significantly improve model performance
5. Polynomial features allow fitting non-linear relationships

### Personal Notes 📝
Today's learning significantly deepened my understanding of the practical aspects of machine learning optimization. The relationship between feature scaling and gradient descent performance was particularly enlightening. I found the systematic approach to choosing learning rates very practical and will definitely use this in future projects.

___
## Day 8
### Topic: Exploratory Data Analysis with Python
*Date: October 10, 2024*

Today's Learning Objectives Completed ✅
- Initial Data Exploration Techniques
- Data Cleaning and Imputation Methods
- Understanding Relationships in Data
- Practical Applications of EDA

### Detailed Notes 📝
![65d3a069871456bd33730869_GC2xT1oWgAA1rAC](https://github.com/user-attachments/assets/b43524f9-8843-49b7-8dcb-e5ed09046cf6)


#### Initial Data Exploration
- Key functions for first look:
  - `.head()`: Preview first few rows
  - `.info()`: Get overview of data types and missing values
  - `.describe()`: Summary statistics for numerical columns
  - `.value_counts()`: Count categorical values
  - `.dtypes`: Check data types

#### Data Cleaning & Imputation
- Handling Missing Data:
  - Detection using `.isna().sum()`
  - Strategies:
    1. Drop if < 5% missing
    2. Impute with mean/median/mode
    3. Group-based imputation
- Outlier Management:
  - Detection using IQR method
  - Visualization with boxplots
  - Decision points: remove, transform, or keep
  - Impact on distribution and analysis

#### Relationships in Data
- Time-based Analysis:
  - Converting to DateTime using `pd.to_datetime()`
  - Extracting components (year, month, day)
  - Visualizing temporal patterns

- Correlation Analysis:
  - Using `.corr()` for numerical relationships
  - Visualization with heatmaps
  - Understanding correlation strength and direction

- Categorical Relationships:
  - Cross-tabulation with `pd.crosstab()`
  - KDE plots for distribution comparison
  - Categorical variables in scatter plots using hue

#### Practical Applications
- Feature Generation:
  - Creating new columns from existing data
  - Binning numerical data with `pd.cut()`
  - Extracting datetime features

- Hypothesis Generation:
  - Avoiding data snooping/p-hacking
  - Using EDA to form testable hypotheses
  - Understanding limitations of exploratory analysis

#### Key Takeaways 🔑
- EDA is crucial first step in data science workflow
- Balance between cleaning and analysis is important
- Visualization helps identify patterns and relationships
- Always consider statistical significance
- EDA should lead to actionable insights or hypotheses


___
## Day 9
### Topic: Simple Linear Regression in Python & R
*Date: October 11, 2024*

**Today's Learning Objectives Completed ✅**
- Mastered Simple Linear Regression concepts
- Understood Ordinary Least Squares (OLS) method
- Implemented regression in both Python and R
- Visualized and analyzed training/test results

### Detailed Notes 📝

**Simple Linear Regression Fundamentals**
- Linear regression predicts continuous output (y) based on input features (x)
- Model equation: ŷ = b₀ + b₁x
  - b₀: y-intercept (bias)
  - b₁: slope (coefficient)
- Used for predicting numerical values (e.g., salary based on years of experience)

**Ordinary Least Squares (OLS) Method**
- Goal: Minimize sum of squared residuals
- Residual: Difference between actual (yᵢ) and predicted (ŷᵢ) values
- Formula: minimize Σ(yᵢ - ŷᵢ)²
- Finds optimal values for b₀ and b₁ that best fit the data


![image](https://github.com/user-attachments/assets/c3a33019-84b6-4a70-ad0d-8f16fe5f320e)


**Implementation Highlights**

**Python Implementation:**
```python
# Key steps:
1. Data preprocessing
   - Loaded salary data using pandas
   - Split features (X) and target (y)

2. Handling missing values
   - Used SimpleImputer with mean strategy

3. Train-test split
   - 70-30 split ratio
   - Random state set for reproducibility

4. Model training
   - Used sklearn's LinearRegression
   - Fitted on training data

5. Visualization
   - Created scatter plots with seaborn
   - Added regression line for predictions
```
![image](https://github.com/user-attachments/assets/9812b24a-8f55-4640-87a3-80d5d9755cc3)
![image](https://github.com/user-attachments/assets/60bc32fd-bc77-466d-ac6e-df464516cddf)


**R Implementation:**
```r
# Key steps:
1. Data loading and splitting
   - Used caTools for splitting
   - 70-30 ratio maintained

2. Model fitting
   - Used lm() function
   - Formula: Salary ~ YearsExperience

3. Visualization
   - Used ggplot2 for plotting
   - Created separate plots for training and test sets
```
![image](https://github.com/user-attachments/assets/5487e5c4-20ff-409b-9b4e-087a339e9a51)
![image](https://github.com/user-attachments/assets/02696e67-6791-41ec-81c6-92ba9645ac23)
![image](https://github.com/user-attachments/assets/eb6e9fff-12bf-40b0-bb71-aebab8e85da1)

**Key Insights 💡**
- Linear regression works well for this salary prediction case
- The relationship between experience and salary is approximately linear
- Model generalizes well from training to test data
- Both Python and R implementations showed similar results


___
## Day 10
### Topic: Multiple Linear Regression in Python
*Date: October 12, 2024*

**Today's Learning Objectives Completed ✅**
- Mastered Multiple Linear Regression in Python with sklearn
- Trained a model to predict startup profits based on multiple features
- Visualized model performance using KDE plots
- Integrated the trained model into a Next.js frontend with a Flask backend into a [VC Profit Predictor](https://vc-profit-predictor.vercel.app/) web application

### Detailed Notes 📝

**Multiple Linear Regression Implementation**

I implemented a multiple linear regression model to predict startup profits based on various features:
- R&D Spend
- Administration Spend
- Marketing Spend
- State (categorical feature)

Key steps in the implementation:

a) Data Preprocessing:
```python
dataset = pd.read_csv("50_Startups.csv")
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
```

b) Handling Missing Values:
```python
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, :-1])
X[:, :-1] = imputer.transform(X[:, :-1])
```

c) Encoding Categorical Data:
```python
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
```

d) Train-Test Split:
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

e) Model Training:
```python
regressor = LinearRegression()
regressor.fit(X_train, y_train)
```

**Model Visualization and Evaluation**

I used KDE plots to visualize the model's performance on unseen data:

```python
sns.set_theme(style="darkgrid")
sns.kdeplot(y_test, color="red", label="Actual Values")
sns.kdeplot(y_hat, color="blue", label="Fitted Values")
plt.title("Actual v/s Fitted Values (Test Set)")
plt.xlabel("Value")
plt.ylabel("Density")
plt.legend()
plt.show()
```

![image](https://github.com/user-attachments/assets/16bc858c-6110-4472-972e-45df936c24a0)

![image](https://github.com/user-attachments/assets/477ad37a-6504-40dd-90cf-ce666ac05d6e)


The KDE plot shows a close alignment between actual and predicted values, indicating good model performance.

**VC Profit Predictor Web Application**

I integrated the trained model into a web application using Next.js for the frontend and Flask for the backend.

**Key features of the application:**
- Input fields for R&D Spend, Administration Spend, Marketing Spend, and State
- Prediction of potential profit based on input values
- Sample data buttons for quick testing
- Clear explanation of the application's purpose and usage

**Screenshots of the application:**

*The main interface of the VC Profit Predictor, showing input fields and prediction result*

![image](https://github.com/user-attachments/assets/e9a17e7c-6568-4091-8aa3-73806e62d940)

*Sample data feature for quick testing of different scenarios*

![image](https://github.com/user-attachments/assets/ec34b64c-967f-4923-93f2-1784741ae975)

**Key Insights 💡**
- Multiple linear regression allows us to consider various factors affecting startup profitability
- The model shows good performance on unseen data, as visualized by the KDE plot
- Integrating ML models into web applications provides an accessible interface for non-technical users
- This tool can help VCs make data-driven investment decisions by analyzing spending patterns and regional variations
- This project demonstrates the practical application of machine learning in a business context, showcasing how data science can inform investment strategies in the venture capital world.


___
## Day 11
### Topic: Advanced Techniques in Multiple Linear Regression
*Date: October 13, 2024*

**Today's Learning Objectives Completed ✅**
- Mastered the concept of dummy variables in regression
- Understood the dummy variable trap and how to avoid it
- Explored statistical significance and p-values in model evaluation
- Learned various model building techniques
- Studied score comparison methods for model evaluation
- Implemented Multiple Linear Regression Model in R
- Completed [VC Profit Predictor](https://github.com/trishan9/VC-Profit-Predictor) Project

### Detailed Notes 📝

### Dummy Variables

- **Definition**: Dummy variables are used to convert categorical data into a numerical format that can be easily used in regression models.
- **Purpose**: They allow the inclusion of categorical variables (like gender, city, or type) in regression models by representing them with numbers.
- **How It Works**:
  - Categorical variables are converted into multiple binary (0 or 1) variables.
  - For a categorical variable with `n` categories, we create `n-1` dummy variables.

![image](https://github.com/user-attachments/assets/ac8724f9-6c5d-4e7d-80c2-4167995f219d)


### Example
Suppose you have a categorical variable called "City" with three categories: Kathmandu, Pokhara, and Lalitpur.
- Create two dummy variables:
  - **Dummy 1**: 1 if Kathmandu, 0 otherwise.
  - **Dummy 2**: 1 if Pokhara, 0 otherwise.
- If both dummy variables are 0, it automatically represents Lalitpur.

### Dummy Variable Trap

- **Definition**: The Dummy Variable Trap occurs when two or more dummy variables are highly correlated (multicollinear), which leads to redundancy in the data.
- **How It Happens**:
  - If you create a dummy variable for each category of a categorical variable (e.g., Kathmandu, Pokhara, Lalitpur), this results in perfect multicollinearity.
  - This means that one dummy variable can be predicted from the others, causing issues in regression models.

![image](https://github.com/user-attachments/assets/a3670aff-2392-473b-b3a8-ee9e870adbea)


#### Example of the Dummy Variable Trap
Imagine you have a regression model with a categorical variable "City" that has three categories: Kathmandu, Pokhara, and Lalitpur.

If you create three dummy variables as follows:
- `Dummy 1` = 1 if Kathmandu, 0 otherwise.
- `Dummy 2` = 1 if Pokhara, 0 otherwise.
- `Dummy 3` = 1 if Lalitpur, 0 otherwise.

In this setup, there's a problem:
- If Dummy 1 and Dummy 2 are both 0, then we know for sure that Dummy 3 must be 1.
- This creates a linear relationship between the dummy variables (`Dummy 3 = 1 - Dummy 1 - Dummy 2`), leading to multicollinearity.

#### Solution to Avoid the Dummy Variable Trap
- **Always use `n-1` dummy variables for `n` categories.**
  - For the example above, we should use only two dummy variables instead of three.
  - Possible setup:
    - `Dummy 1` = 1 if Kathmandu, 0 otherwise.
    - `Dummy 2` = 1 if Pokhara, 0 otherwise.
    - The absence of both (i.e., 0 for both Dummy 1 and Dummy 2) would mean the observation is for Lalitpur.

### Intuition Behind the Dummy Variable Trap
- The trap happens because including all the dummy variables introduces redundancy.
- It leads to issues in interpreting the regression coefficients and reduces the statistical significance of predictors.
- By avoiding the trap, you ensure that the model runs properly and gives more reliable results.

### Statistical Significance Level

- **Definition**: A significance level is a threshold that determines how confident we are in rejecting the null hypothesis for a predictor in the regression model.
- **Common Levels**: Commonly used levels include 5% (0.05), 1% (0.01), and 10% (0.10).
  - A 5% significance level means we are 95% confident that the predictor's effect on the target variable is not due to chance.

### P-Value

- **Definition**: The p-value measures the probability that the observed relationship between the predictor and the target variable occurred by random chance.
- **Interpretation**:
  - If **p-value < significance level**: Reject the null hypothesis, indicating that the predictor is statistically significant.
  - If **p-value >= significance level**: Fail to reject the null hypothesis, meaning that the predictor does not have a significant effect on the target variable.

### Intuition Behind Statistical Significance
- It helps to decide whether a predictor variable genuinely impacts the target variable or if the observed effect could have occurred by chance.
- A lower p-value indicates that the predictor is more likely to have a meaningful contribution to the model.

### Building a Model

Building an effective regression model involves selecting the right predictor variables. There are several common approaches to model building:
### 1. All-in Approach

- **Definition**: This approach involves including all available predictors in the regression model regardless of their significance.
- **Use Cases**:
  - Useful when you have prior knowledge or domain expertise that indicates all variables play an essential role.
  - When you're conducting exploratory data analysis and want to assess the collective effect of all predictors.
- **Drawback**: Including irrelevant predictors can lead to overfitting, where the model fits the training data too well and performs poorly on new data.

### 2. Backward Elimination

- **Process**:
  1. Start with all the predictors in the model.
  2. Identify the predictor with the highest p-value greater than the chosen significance level.
  3. Remove this predictor from the model.
  4. Refit the model and repeat the process until all remaining predictors are statistically significant.
- **Benefit**: It systematically reduces model complexity by eliminating less relevant variables.
- **Intuition**: Focuses on simplifying the model by removing predictors that do not provide meaningful information.

![image](https://github.com/user-attachments/assets/7f22ce70-bccc-496a-81d3-99dec934ae9b)


### 3. Forward Selection

- **Process**:
  1. Start with no predictors in the model.
  2. Add the predictor with the lowest p-value that improves the model's performance.
  3. Continue adding predictors one by one until no significant improvement is observed.
- **Use Case**: Ideal when you have a large number of predictors and want to build a simple model starting from scratch.
- **Drawback**: Forward selection might miss some combinations of predictors that could become significant when considered together.

### 4. Bidirectional Elimination

- **Definition**: Combines Forward Selection and Backward Elimination in one method.
- **Process**: At each step, it adds significant predictors and removes those that become non-significant.
- **Benefit**: It evaluates the significance of variables as you add and remove them, providing a balanced approach to model building.
- **Intuition**: Ensures that as new variables are introduced, existing variables are still relevant to the model.

### Score Comparison
To evaluate and compare different regression models, the following metrics are most commonly used:

### 1. R-squared (Coefficient of Determination)

- **Definition**: Measures the proportion of variance in the target variable that can be explained by the predictor variables.
- **Range**: 0 to 1 (or 0% to 100%).
  - **Closer to 1** indicates that a larger portion of the variance is explained by the model.
  - **Closer to 0** suggests that the model does not explain much of the variability in the data.
- **Limitations**: Adding more predictors always increases R-squared, even if those predictors are not relevant.

### 2. Adjusted R-squared

- **Definition**: Adjusted R-squared modifies the R-squared value to account for the number of predictors in the model.
- **Interpretation**:
  - It only increases if the newly added predictor improves the model more than would be expected by chance.
  - If a predictor does not improve the model, Adjusted R-squared decreases.
- **Use Case**: It helps to avoid overfitting by ensuring that each added predictor genuinely contributes to improving the model.

### Intuition for Score Comparison

- **R-squared** tells you how well the model fits the data overall.
- **Adjusted R-squared** prevents you from adding irrelevant predictors by adjusting for model complexity.
- Use these metrics to select the best model that balances simplicity and predictive power.

### Multiple Linear Regression Implementation in R
![image](https://github.com/user-attachments/assets/ebcdc439-2382-4223-8ece-92b4b69e5123)

### VC Profit Predictor Project Completion
https://github.com/user-attachments/assets/06c969d9-e93a-4e24-bc23-8d50282cb001

### Key Insights 💡
- Proper handling of categorical variables is crucial for accurate regression models
- Statistical significance helps identify truly impactful predictors
- Different model building techniques suit various scenarios and data characteristics
- Balancing model complexity and predictive power is key in regression analysis


___
## Day 12
### Topic: Polynomial Regression
*Date: October 14, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding the need for Polynomial Regression
- Mathematical foundations of Polynomial Regression
- How Polynomial Regression works
- Intuition behind Polynomial Regression
- Example and practical considerations

### Detailed Notes 📝

### Why Polynomial Regression?

![image](https://github.com/user-attachments/assets/592c4619-8ce0-4d70-b0ec-da117ac88679)

#### Linear Regression Limitations
- Linear regression assumes a straight-line relationship between features and target
- Real-world data often has non-linear patterns

#### Non-linear Relationships
- Polynomial regression is useful when data points form a curve that cannot be adequately described by a linear model.
- It helps to capture the turning points, bends, and curves in the data.

#### Model Flexibility
- Adding polynomial terms allows for more flexible adaptation to data trends

### Mathematics Behind Polynomial Regression

![image](https://github.com/user-attachments/assets/3359fc29-9ada-4315-b15c-2375fe16fad3)


Polynomial equation:
Polynomial regression is a special case of multiple linear regression because we still use a linear equation, but with polynomial features. The polynomial equation is given by:

y = β₀ + β₁x + β₂x² + β₃x³ + ⋯ + βₙxⁿ

Where:
- y: dependent variable
- x: independent variable
- β₀, β₁, β₂, ..., βₙ: coefficients
- n: degree of the polynomial

### How Does It Work?

- Transforms original features into higher-degree polynomial features
- Applies linear regression to fit these transformed features
- Example: x → x, x², x³, ..., xⁿ

### Intuition Behind Polynomial Regression

- The idea behind polynomial regression is that by adding higher-degree terms, we can better fit the training data by capturing more of its variations.
- **Low Degree**: Underfitting might occur if the polynomial degree is too low to capture the data's trends.
- **High Degree**: Overfitting might occur if the polynomial degree is too high, causing the model to fit the training data too well, including the noise.

### Example of Polynomial Regression

Step-by-step Example:
1. **Original Data**: Let's say our data points look like a quadratic curve.
2. **Choosing the Polynomial Degree**: We choose a degree of 2, which means the equation becomes:
   y = β₀ + β₁x + β₂x²
3. **Fitting the Model**: We fit this quadratic equation to our data using linear regression techniques.
4. **Prediction**: We can now make predictions using the fitted polynomial curve.

#### Underfitting and Overfitting:
- **Underfitting**: If the degree of the polynomial is too low, the model will not capture the true trend in the data.
- **Overfitting**: If the degree is too high, the model will fit the noise in the training data and perform poorly on unseen data.

#### Choosing the Right Degree
- Experiment with different degrees of the polynomial to find the best fit.
- Use metrics like Mean Squared Error (MSE) or R-squared to compare model performance.

### Implementation of Polynomial Regression
**Python**

![image](https://github.com/user-attachments/assets/2f90794a-8804-49c6-b853-31b4f97ebb4c)
![image](https://github.com/user-attachments/assets/73bd1480-d88d-49a8-a5a7-259123d6ca16)

**R**

![image](https://github.com/user-attachments/assets/6f283cab-2d14-4988-b3e4-5d26c41cf2e2)
![image](https://github.com/user-attachments/assets/ffe99ca7-242b-4ec7-bd58-1e5a2e1844d6)


### Key Insights 🔑
- Polynomial regression captures non-linear trends in data
- It's an extension of linear regression with higher-degree terms
- Proper degree selection is crucial to avoid under/overfitting
- Visual representation aids in understanding model performance
- Balancing complexity and generalization is key to a good model


___
## Day 13
### Topic: Bias-Variance Trade-off
*Date: October 15, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding the concept of Bias-Variance Trade-off
- Exploring key components: Bias and Variance
- Analyzing the effects of high bias and high variance
- Learning strategies to manage the trade-off
- Studying examples in different ML models

### Detailed Notes 📝

### Introduction to Bias-Variance Trade-off

![image](https://github.com/user-attachments/assets/c2b2e215-cf26-45e4-900b-3e2c293e0ff5)

- Fundamental concept in machine learning
- Describes the balance between two sources of errors: bias and variance
- Crucial for building models that generalize well to new, unseen data

### Key Concepts

#### Bias
- inability of ML model to truly capture relationships in training data.
- Bias refers to the error introduced by the simplifying assumptions a model makes to learn from the training data.
- High bias means that the model is too simple and makes assumptions that may not align well with the data, leading to **underfitting**.
- A model with high bias pays little attention to the training data, oversimplifies the problem, and misses relevant relationships between the features and the target outputs.

#### Variance
- difference of the fits between different datasets.
- Variance refers to the error introduced by the model's sensitivity to the small fluctuations in the training data.
- High variance means that the model is too complex and captures the noise in the training data along with the actual data patterns, leading to **overfitting**.
- A model with high variance pays too much attention to the training data, learns its details and noise, and fails to generalize well to new data.

### The Trade-off

- The goal in machine learning is to develop a model that has both low bias and low variance, but in practice, reducing one often increases the other.
- The **Bias-Variance Trade-off** refers to the challenge of finding the right balance between bias and variance to minimize the **total error**.

### Effects of High Bias and High Variance

![image](https://github.com/user-attachments/assets/72bd0c8e-4b73-4439-9bc5-2754e0c5b6c5)

#### High Bias (Underfitting)

![image](https://github.com/user-attachments/assets/b8b77e16-5eb0-41c4-97c8-8341fe19050b)

- Model is too simple.
- Has low complexity.
- Does not capture the underlying patterns in the data.
- Leads to high training error and high testing error.
- Examples: Linear Regression on a nonlinear dataset, or a shallow decision tree.

#### High Variance (Overfitting)

![image](https://github.com/user-attachments/assets/8fcf10ec-6c34-4d54-a828-98362d8e9960)

- training error is extremely low but test error is high.
- Model is too complex.
- Has high flexibility.
- Captures noise or irrelevant patterns in the training data.
- Leads to low training error but high testing error.
- Examples: Deep decision trees, k-NN with very low values of k.

### Finding the Optimal Balance

![image](https://github.com/user-attachments/assets/3d1a705c-a0dc-4ad7-aacd-8a23f50f3ef6)

To achieve the best performance, a model should have:
- Low enough bias to make predictions that are on average correct.
- Low enough variance to generalize well to new data.

**Steps to Find the Balance:**
1. **Start Simple and Increase Complexity:** Begin with a simple model and increase complexity gradually to observe how the model performs on training and validation sets.
2. **Use Cross-Validation:** Split your data into multiple training and testing sets to better understand how well the model generalizes.
3. **Regularization Techniques:** Use techniques like Lasso, Ridge, or Elastic Net to prevent overfitting by penalizing large coefficients in the model.
4. **Feature Engineering:** Select the most relevant features to reduce complexity and noise in the data.
5. **Ensemble Methods:** Use methods like Bagging, Boosting, or Stacking to combine predictions from multiple models to reduce bias and variance.

### Examples in ML Models

1. **Linear Regression:**
   - High Bias: Assumes a linear relationship, which may not fit nonlinear data well.
   - Low Variance: Small changes in the data do not drastically change the predictions.

2. **Polynomial Regression:**
   - Low Bias: Can fit complex patterns in the data.
   - High Variance: Overfits the data if the polynomial degree is too high.

### Strategies to Manage Bias-Variance Trade-off

1. **Adjust Model Complexity:** Increase complexity to reduce bias or decrease complexity to reduce variance.
2. **Regularization Techniques:**
   - L1 Regularization (Lasso): Shrinks coefficients, reducing variance.
   - L2 Regularization (Ridge): Distributes error among features, reducing variance.
3. **Use More Training Data:** Helps to reduce variance without affecting bias significantly.
4. **Ensemble Methods:**
   - **Bagging:** Reduces variance by averaging multiple models (e.g., Random Forest).
   - **Boosting:** Reduces bias by sequentially building models that correct the errors of the previous ones.

### Key Insights 🔑
- Bias leads to underfitting, variance leads to overfitting
- The goal is to find a balance between simplicity and complexity
- Techniques like cross-validation, regularization, and ensemble methods help manage the trade-off
- Understanding this concept is crucial for building robust, generalizable ML models
- Different models have different bias-variance characteristics
- The optimal model minimizes both training and validation errors


___
## Day 14
### Topic: Ridge & Lasso Regression
*Date: October 16, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Ridge Regression (L2 regularization)
- Exploring Lasso Regression (L1 regularization)
- Comparing Ridge and Lasso Regression techniques
- Learning about Elastic Net
- Analyzing practical insights from regularization techniques

### Detailed Notes 📝

Ridge Regression and Lasso Regression are both types of linear regression techniques used to address multicollinearity and overfitting in machine learning models. They are regularization methods that add a penalty to the regression model to reduce the magnitude of the coefficients.

### Ridge Regression (L2 Regularization)
Ridge Regression, also known as **L2 regularization**, adds a penalty equal to the sum of the squared values of the coefficients (except the intercept) to the cost function. It aims to shrink the coefficients of less important features closer to zero without completely removing them.

- Also known as L2 regularization
- Adds penalty equal to the sum of squared coefficients to the cost function
- Cost Function: RSS + λ ∑(β_j^2)
  - RSS: Residual Sum of Squares
  - λ: Regularization parameter
  - β_j: Coefficients of features

#### Effect of the Penalty

When **λ** is increased, the coefficients of the less significant features shrink towards zero, but they never reach exactly zero. Ridge regression is useful when many predictors are correlated.

#### Impact

Ridge Regression performs well when all the predictor variables are important but may have multicollinearity issues. It helps stabilize the model by reducing variance.

#### Example

In practice, Ridge Regression is used to deal with datasets where features are correlated. For instance, if you have features like "square footage" and "number of rooms" in predicting house prices, Ridge Regression helps to stabilize the model coefficients and prevent overfitting.

![image](https://github.com/user-attachments/assets/d7d1d64d-2ac1-49c7-99ad-42a99348b8d7)
![image](https://github.com/user-attachments/assets/20df953d-57a0-46e0-aab0-2c046fbbb970)

### Lasso Regression (L1 Regularization)
Lasso Regression, short for **Least Absolute Shrinkage and Selection Operator** or **L1 regularization**, adds a penalty equal to the absolute values of the coefficients to the cost function. Unlike Ridge Regression, Lasso Regression can shrink some coefficients to zero, effectively performing feature selection.

- Least Absolute Shrinkage and Selection Operator
- Adds penalty equal to the absolute values of coefficients to the cost function
- Cost Function: RSS + λ ∑|β_j|

#### Effect of the Penalty

Lasso Regression forces some coefficients to become exactly zero when the regularization parameter is large enough. This makes Lasso useful for feature selection, as it automatically excludes irrelevant features from the model.

#### Impact

Lasso Regression works well when there are many predictors, but only a subset of them are actually relevant for predicting the target variable. It helps simplify the model and reduce overfitting.

#### Example

Lasso Regression is particularly useful in scenarios where feature selection is crucial. For example, in genetics, if you have thousands of genetic markers, Lasso can automatically select only the few markers that have a significant impact on the disease risk prediction.

![image](https://github.com/user-attachments/assets/e2e481f6-6d77-49d8-bcb9-4ce5ef49061d)

### Ridge vs Lasso Regression

| **Feature**             | **Ridge Regression (L2)**                    | **Lasso Regression (L1)**                    |
|-------------------------|---------------------------------------------|---------------------------------------------|
| **Penalty Type**        | Squared magnitude of the coefficients        | Absolute magnitude of the coefficients      |
| **Feature Selection**   | Does not perform feature selection           | Can perform feature selection by shrinking coefficients to zero |
| **Coefficient Shrinkage**| Shrinks coefficients towards zero but not exactly zero | Can shrink coefficients exactly to zero    |
| **Use Case**            | When all features are important but might be collinear | When only a few features are important     |

### Choosing Between Ridge and Lasso Regression
- Use **Ridge Regression** when you expect all features to be relevant, but some might be correlated. It helps to control overfitting by penalizing large coefficients.
- Use **Lasso Regression** if you suspect that only a few predictors have significant influence on the response variable. Lasso's ability to perform feature selection can lead to a simpler, more interpretable model.

### Elastic Net
Sometimes, a combination of Ridge and Lasso Regression can be used, called **Elastic Net**. Elastic Net is useful when there are multiple correlated features, as it balances the strengths of both Ridge and Lasso Regression. It combines the penalties of both L1 and L2 regularization:

- Combination of Ridge and Lasso Regression
- Cost Function: RSS + λ1 ∑|β_j| + λ2 ∑(β_j^2)
- Balances strengths of both Ridge and Lasso
- Useful when multiple correlated features exist

### Practical Insights

1. **Ridge Regression**:
   - Useful when all predictors are important.
   - Helps mitigate the issue of multicollinearity.
   - Coefficients never become exactly zero, hence all features are retained in the model.

2. **Lasso Regression**:
   - Ideal for automatic feature selection and simpler models.
   - Forces some coefficients to be exactly zero, eliminating irrelevant features.
   - Suitable when you expect only a subset of features to be impactful.

### Key Insights 🔑
- Ridge and Lasso are regularization techniques to address multicollinearity and overfitting
- Ridge shrinks coefficients towards zero, Lasso can shrink them to exactly zero
- Lasso performs feature selection, while Ridge retains all features
- Elastic Net combines both approaches for balanced regularization
- Choice between Ridge and Lasso depends on the specific problem and data characteristics
- Regularization techniques help in creating more stable and interpretable models


___
## Day 15
### Topic: Ridge & Lasso Regression Implementation in Python
*Date: October 17, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Ridge and Lasso Regression
- Implementing Ridge and Lasso Regression using scikit-learn
- Hyperparameter tuning with GridSearchCV
- Evaluating model performance

### Detailed Notes 📝

### Ridge and Lasso Regression
- Both are regularized versions of linear regression
- Help prevent overfitting by penalizing large coefficients
- Implemented using scikit-learn library in Python

#### Ridge Regression
- Uses L2 regularization
- Shrinks coefficients but does not eliminate them
- Helps reduce multicollinearity

#### Lasso Regression
- Uses L1 regularization
- Can drive some coefficients to zero, performing feature selection
- Useful for identifying important features

### Implementation Steps
1. Import necessary libraries (sklearn, numpy)
2. Load and split data
3. Perform hyperparameter tuning using GridSearchCV
4. Train the model with best hyperparameters
5. Evaluate model performance using Mean Squared Error, and Plots

### Key Concepts
- Regularization strength (alpha/lambda): Controls the penalty on model complexity
- GridSearchCV: Automates hyperparameter tuning using cross-validation
- Mean Squared Error (MSE): Measures the average squared difference between predictions and actual values

### Code Snippets
```python
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Hyperparameter tuning
param_grid = {'alpha': np.logspace(-4, 4, 50)}
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Train and evaluate
best_model = model(alpha=best_alpha)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
```

### Visualizing Results
- KDE (Kernel Density Estimate) plot used to visualize prediction errors
- Helps understand the distribution of residuals

![image](https://github.com/user-attachments/assets/c57077e8-897a-4888-9ad8-ed476b1a6fc8)

### Key Differences between Ridge and Lasso
- Ridge: Shrinks coefficients, doesn't perform feature selection
- Lasso: Can eliminate less important features by setting coefficients to zero

### Key Takeaways 🔑
- Ridge and Lasso are powerful techniques for preventing overfitting in linear regression
- Hyperparameter tuning is crucial for optimal model performance
- Lasso can perform feature selection, while Ridge helps with multicollinearity
- Visualization of residuals provides insights into model performance
- Both methods balance model complexity and accuracy


___
## Day 16
### Topic: ElasticNet Regression & SVR Deepdive
*Date: October 18, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding ElasticNet Regression and its comparison with Ridge and Lasso
- Learning about Support Vector Regression (SVR)
- Exploring model evaluation metrics (R2 score, MSE, RMSE)
- Practicing Basic hyperparameter tuning using GridSearchCV

### Detailed Notes 📝

### ElasticNet Regression
- Combines L1 (Lasso) and L2 (Ridge) regularization
- Balances feature selection (Lasso) and handling correlated predictors (Ridge)
- Formula: minimizes (RSS + α * [(1-ρ) * L2 + ρ * L1])
  - RSS: Residual Sum of Squares
  - α: regularization strength
  - ρ: balance between L1 and L2 (0 ≤ ρ ≤ 1)

### ElasticNet Summary from the [Paper](https://bmcproc.biomedcentral.com/articles/10.1186/1753-6561-6-S2-S10)

The paper discusses **genomic selection (GS)**, a method used in plant and animal breeding to predict breeding values based on genetic information. Accurate predictions help breeders make better choices about which plants or animals to select for breeding.

**The Importance of Regression Methods**

To predict breeding values, researchers use different statistical techniques. The paper evaluates three regularized linear regression methods: **Ridge Regression**, **Lasso Regression**, and **Elastic Net Regression**. These methods help manage datasets where the number of predictors (variables) is much larger than the number of observations (data points), which is common in genomics.

**1. Ridge Regression**

- **What It Is:** Ridge regression adds a penalty to the size of the coefficients (the numbers that represent the effect of each predictor). This penalty helps to keep the model stable and prevents the coefficients from becoming too large when there are many predictors.
- **How It Works:** It minimizes the error of the prediction while also trying to keep the coefficients small by adding a penalty based on their squares.
- **Strengths:** It works well when there are many predictors that are correlated with each other. Instead of choosing some predictors and ignoring others, it shrinks all the coefficients toward zero but doesn't eliminate them.
- **Weaknesses:** It doesn’t automatically select the most important predictors; all are included in the model.

**2. Lasso Regression**

- **What It Is:** Lasso regression also adds a penalty, but this one is based on the absolute values of the coefficients. This characteristic encourages the model to shrink some coefficients to exactly zero, effectively selecting a simpler model.
- **How It Works:** It minimizes the prediction error while adding a penalty based on the absolute values of the coefficients. This causes some coefficients to become zero, which means those predictors are excluded from the model.
- **Strengths:** It automatically selects relevant predictors, making it easier to interpret the model. This is especially useful in high-dimensional datasets, like those in genomic studies.
- **Weaknesses:** If the predictors are highly correlated, lasso tends to pick one and ignore the others, which might not always be the best choice.

**3. Elastic Net Regression**

- **What It Is:** Elastic Net combines the strengths of both Ridge and Lasso. It uses both types of penalties, which allows it to handle correlations among predictors better than Lasso alone.
- **How It Works:** It minimizes prediction error with penalties based on both the squares (Ridge) and absolute values (Lasso) of the coefficients. This means it can shrink coefficients and also set some to zero.
- **Strengths:** It performs well when predictors are highly correlated and is better at selecting groups of correlated variables. This makes it more flexible and robust in genomic selection.
- **Weaknesses:** While it can select groups of variables, it does not guarantee optimal model selection like some other methods.

**Key Findings from the Paper**

![image](https://github.com/user-attachments/assets/2121eb22-7b56-4a3f-81de-e5b18dd89e2b)

- **Performance:** The paper found that Elastic Net, Lasso, and Adaptive Lasso had similar accuracy levels, and they performed better than Ridge Regression and Ridge Regression BLUP. This means that when trying to predict breeding values using genomic data, Lasso and Elastic Net methods were more effective.
- **Practical Implications:** For breeders, using these more advanced regression techniques can lead to better predictions of breeding values, helping them select the best candidates for breeding. This can improve the efficiency of breeding programs.

### ElasticNet Implementation in Python
![image](https://github.com/user-attachments/assets/1631f301-bab0-4b29-a118-173ebe3a54c8)


### Support Vector Regression
Support Vector Regression (SVR) is a type of machine learning algorithm used for regression tasks. It builds upon the principles of Support Vector Machines (SVM), which are primarily known for classification tasks, but can also be adapted for predicting continuous outcomes, which is the goal of regression.

![1_F0SvFUJxql-H1hYW0j57eA](https://github.com/user-attachments/assets/5b2d4f2c-6b94-46e1-8d79-1bfd82a42632)

![image](https://github.com/user-attachments/assets/45b46aa1-c1a8-47ca-a2dc-99bbbe8c0a34)

### Insights from the [Paper](https://core.ac.uk/download/pdf/81523322.pdf):

#### **Support Vectors**
- In SVR, support vectors are the data points that are closest to the predicted regression line (or hyperplane) but do not fall within the margin of the insensitive tube. They play a crucial role in determining the position of the regression line. Think of them as the critical data points that help define the boundary of the prediction.
- These are the data points that are most important in defining the model. They lie closest to the predicted function and help shape it.

**Example:** If you imagine plotting house prices against their sizes, the points closest to the line (the predicted prices) that help define how the line should slope are your support vectors.

#### **Insensitive Tube (Epsilon Tube)**
- SVR introduces a concept called the "e-insensitive tube." This is a margin around the predicted function where errors (the differences between predicted and actual values) are not penalized if they fall within a certain range. This means that small errors are ignored, making the model robust against noise in the data.
- Insensitive tube, often referred to as the epsilon tube (ε-tube), is a zone around the predicted regression line where errors are ignored. This means if predictions fall within this tube, they do not count as errors. The width of the tube is defined by the epsilon parameter (ε).

**Use Case:** If you're predicting house prices, you might say that any prediction within $5,000 of the actual price is acceptable. So, if the actual price is $300,000, as long as the predicted price is between $295,000 and $305,000, it's considered good.

#### **Kernel Trick**
SVR can use a technique called the kernel trick to handle non-linear data. This means it can transform the input data into a higher-dimensional space where a linear regression line can fit more appropriately. Different types of kernels (like polynomial, radial basis function) can be used depending on the problem.

**Example:** If you were trying to predict house prices and your data shows a non-linear relationship (like prices increasing at an increasing rate as size increases), a kernel trick could help find a better fit.

#### How SVR Works

1. **Training Phase**: The SVR algorithm takes the training data and attempts to find the best-fit line (or hyperplane in higher dimensions) within the epsilon tube. It identifies the support vectors and the parameters ε and C (regularization parameter that balances the trade-off between maximizing the margin and minimizing the prediction error).

2. **Prediction Phase**: For new data, SVR uses the learned regression line to predict values. If the predicted value falls within the ε-tube, it is considered an acceptable prediction; if it falls outside, it is subject to slack variables and may incur a penalty in the model training.

#### Advantages of SVR
- Flexibility: SVR can handle both linear and non-linear relationships through its kernel functions.
- Robustness: The e-insensitive loss function makes it less sensitive to outliers or noisy data points.
- Generalization: SVR generally performs well on unseen data due to its ability to find a balance between fitting the training data and maintaining simplicity.

#### Summary

In summary, Support Vector Regression is a powerful technique for regression tasks that focuses on finding a balance between prediction accuracy and complexity. By utilizing concepts like support vectors, insensitive tubes, slack variables, and the kernel trick, it allows for effective modeling of both linear and non-linear relationships.

### SVR Implementation in Python
![image](https://github.com/user-attachments/assets/443f758e-3132-444f-b725-bba335b33d44)

### Key Takeaways 🔑
- ElasticNet combines strengths of Lasso and Ridge, suitable for correlated predictors
- SVR extends SVM concepts to regression tasks, handling non-linear relationships
- Proper model evaluation and hyperparameter tuning are crucial for optimal performance
- R2, MSE, and RMSE provide different perspectives on model accuracy
- GridSearchCV automates the process of finding the best hyperparameters


___
## Day 17
### Topic: Decision Tree Regression
*Date: October 19, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding the concept and mechanics of Decision Tree Regression
- Exploring the process of building a decision tree for regression tasks
- Learning about splitting criteria and prediction methods in decision trees
- Identifying use cases, advantages, and limitations of Decision Tree Regression

### Detailed Notes 📝

### Decision Tree Regression

**Decision Tree Regression** is a machine learning algorithm used to predict continuous values, making it a popular method for regression tasks. It works by creating a tree-like structure of decision rules that split the data into different subsets based on the values of the input features. Let's break down how Decision Tree Regression works, its components, and how it makes predictions.

#### What is Decision Tree Regression?

Decision Tree Regression is a type of regression model that predicts a target value by learning simple decision rules inferred from the data's features. It builds a tree structure where:
- **Nodes** represent conditions based on feature values.
- **Branches** represent the outcomes of those conditions.
- **Leaf nodes** contain the predicted values.

The goal of the tree is to partition the dataset into subsets that have similar target values.

#### How Does It Work?

The process of building a decision tree can be summarized in these steps:
1. **Choose the Best Feature for Splitting:** The algorithm examines all features and different values of those features to determine the best way to split the data. The split is chosen based on a criterion that minimizes the prediction error (often using **Mean Squared Error (MSE)**).

2. **Create Branches Based on Conditions:** It creates branches based on the chosen split, which divides the dataset into more homogenous groups with similar target values.

3. **Repeat the Process:** The splitting continues recursively on each branch, creating a deeper tree structure, until a stopping condition is reached (like a maximum tree depth or a minimum number of samples in a node).

4. **Assign Values at Leaf Nodes:** Once the data cannot be split further, the algorithm assigns the **mean** of the target values in each leaf node as the prediction.

#### Making Predictions

When a new data point is fed into the tree, it travels through the nodes based on the decision conditions until it reaches a leaf node. The prediction for that data point is the **mean** of the target values of the training data that fell into that leaf node.

**Example:**
Consider a dataset for predicting the price of Mo:Mo in different cities of Nepal based on size and location:
- If a decision tree splits the data at **location = Kathmandu**, it creates two branches:
  - **Left branch** for Mo:Mo prices in Kathmandu
  - **Right branch** for Mo:Mo prices outside Kathmandu (e.g., Pokhara, Biratnagar)
- If a new data point corresponds to Mo:Mo in Kathmandu, it will go to the left branch, where the predicted price might be the mean price of Mo:Mo in similar locations within Kathmandu.

#### How are the Conditions Set?

The conditions for splitting nodes are set to minimize the prediction error, typically using **Mean Squared Error (MSE)**. The algorithm tries different values of each feature and chooses the split that results in the greatest reduction in error.

![image](https://github.com/user-attachments/assets/3f240240-856a-40d2-b9c4-16da6a2dcd1c)


#### How is the Predicted Value Set?

The predicted value at each leaf node is usually the **mean** of the target values of all the data points in that node. This means that all samples that fall into the same leaf node will have the same prediction.

![image](https://github.com/user-attachments/assets/224418dc-b37e-42f7-b52e-5d043040c5f2)
![image](https://github.com/user-attachments/assets/7bcfbde9-5e3b-4c92-a352-9db04bd2255a)

**Example:**
If a leaf node contains the prices of Mo:Mo in Kathmandu as Rs. 150, Rs. 160, and Rs. 170, the predicted price for any new Mo:Mo order reaching this node will be:
Predicted value = (150 + 160 + 170) / 3 = Rs. 160

#### Use Cases of Decision Tree Regression

Decision Tree Regression can be used in various real-world scenarios:
- **Predicting House Prices:** Based on features like the number of rooms, size, location, etc.
- **Stock Market Analysis:** To forecast stock prices based on historical data.
- **Weather Prediction:** Estimating temperatures or rainfall based on previous patterns.

#### Advantages

- **Easy to Understand and Interpret:** The tree structure is simple to visualize, making it easy to explain the decision-making process.
- **Non-Linear Relationships:** Can handle complex data relationships and patterns without requiring feature scaling or transformation.
- **Feature Importance:** It highlights which features are most important in making predictions.

#### Limitations

- **Overfitting:** Decision trees can easily overfit the training data if they grow too deep, capturing noise instead of patterns.
- **Piecewise Constant Prediction:** The predictions are not smooth but have a step-like structure because all points in the same leaf node have the same predicted value.
- **Sensitive to Small Changes:** Small changes in the data can lead to different splits, resulting in a completely different tree.

#### How Decision Tree Regression Handles Predictions

- When splitting the data, the goal is to create groups where the target values are as similar as possible.
- The predicted value at the leaf node is calculated using the mean of the target values, as it minimizes the squared error.
- Therefore, **any data point that falls into the same leaf node will get the same predicted value**, which is the mean of that group.

#### Summary of Decision Tree Regression

- **Split Criteria:** The algorithm chooses splits that minimize the prediction error using MSE.
- **Predicted Value:** At each leaf node, the mean of the target values is used to make predictions.
- **Advantages:** Easy to interpret, handles non-linear relationships, and shows feature importance.
- **Limitations:** Prone to overfitting, creates piecewise constant predictions, and is sensitive to data variations.

#### Intuition and Visualization

Like a flowchart where each question narrows down the possibilities until you reach a final answer. The splits in the tree are like a series of yes/no questions that guide you toward a prediction. By using averages in the leaf nodes, Decision Tree Regression provides the best guess based on similar cases.

This makes Decision Tree Regression a powerful and intuitive tool for making predictions, especially when dealing with non-linear data patterns.

### Key Takeaways 🔑
- Decision trees split data recursively to create homogeneous groups
- Predictions are based on the mean value of samples in leaf nodes
- Easy to interpret but can overfit if not properly tuned
- Useful for capturing non-linear patterns in data
- Provides insights into feature importance


___
## Day 18
### Topic: Implementing Decision Tree Regression in Python
*Date: October 20, 2024*

**Today's Learning Objectives Completed ✅**
- Implemented Decision Tree Regression on two different datasets
- Explored the impact of single vs. multi-feature datasets on model performance
- Practiced data loading, preparation, and model evaluation techniques
- Visualized Decision Tree Regression predictions and errors

### Detailed Notes 📝

Today, I implemented a Decision Tree Regression model using Python. This technique is useful for predicting continuous values based on the given features. I worked with two datasets: one for a salary prediction based on position levels and another for housing prices in California.

### Steps Taken

1. **Data Loading**
   - I started by loading the dataset using pandas. The first dataset contained positions, levels, and corresponding salaries.
   - For the second dataset, I fetched California housing data.

2. **Data Preparation**
   - I extracted the features (`X`) and target values (`y`) from the datasets.
   - For the salary dataset, I selected the levels as features and salaries as target values.

3. **Model Implementation**
   - I imported the `DecisionTreeRegressor` from `sklearn`.
   - I created an instance of the model and fitted it to the salary dataset.

4. **Making Predictions**
   - After training, I predicted the salary for a position level of 10.
   - To visualize the results, I plotted the actual salaries against the predicted values using matplotlib.

5. **Model Evaluation**
   - I evaluated the model using the R² score and Mean Squared Error (MSE).
   - The results showed an R² score of 1.0 and an MSE of 0.0, indicating perfect predictions on the training dataset.

   ```python
   print(f"R2 Score: {r2_score(y, regressor.predict(X))}")
   print(f"MSE: {mean_squared_error(y, regressor.predict(X))}")
   ```

6. **Second Dataset Implementation**
   - I then moved on to the California housing dataset.
   - I split the data into training and testing sets using `train_test_split`.

   ```python
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
   ```

7. **Training the Model**
   - I trained the Decision Tree Regressor on the training set and made predictions on the testing set.

8. **Evaluation of Predictions**
   - I visualized the differences between predicted and actual values using seaborn's KDE plot.
   - I calculated the R² score and MSE for the predictions, which resulted in an R² score of approximately 0.57 and an MSE of about 0.54.


### Conclusion
- Implementing Decision Tree Regression allowed me to understand how this model can effectively predict continuous values from different datasets. The perfect results on the salary dataset indicate that the model captured the underlying patterns well, while the California housing data showed a more realistic performance. I found the visualizations helpful for assessing the model's accuracy and making sense of the predictions.
- Decision Tree Regression performs better with larger feature sets
- Multi-feature datasets provide more information for the model to learn from
- Single feature datasets may lead to overfitting or simplistic models

### Key Observations

- Single Feature Dataset (Position Salaries):
  - Perfect R² score (1.0) and MSE (0.0) on training data
  - Indicates potential overfitting to the training data

  ![image](https://github.com/user-attachments/assets/860d18b1-d204-4929-af3f-2fda9ea76ec0)


- Multi-feature Dataset (California Housing):
  - R² score: ~0.57
  - MSE: ~0.54
  - More realistic performance, showing the complexity of the problem

![image](https://github.com/user-attachments/assets/71b5db93-2004-4797-bb00-17df9a65e8fa)


### Key Takeaways 🔑

- Decision trees can capture complex patterns in multi-feature datasets
- Perfect scores on training data may indicate overfitting
- Visualization helps in understanding model performance and errors
- R² score and MSE provide quantitative measures of model accuracy
- Decision Tree Regression is more suitable for datasets with multiple features


___
## Day 19
### Topic: Random Forest Regression & All Regression Model Comparison
*Date: October 21, 2024*

**Today's Learning Objectives Completed ✅**
- Gained in-depth understanding of Random Forest Regression
- Implemented Random Forest Regression in Python
- Compared various regression models on the same dataset
- Evaluated models using R-squared and KDE plots

### Detailed Notes 📝

Random Forest regression is a powerful machine learning algorithm used for making predictions, especially when working with complex datasets. It builds upon a technique called Decision Trees. Let's break down the concepts in a simple and detailed manner:

![image](https://github.com/user-attachments/assets/a80373a8-9b4c-41f6-b78e-49803d023260)

![1_ZFuMI_HrI3jt2Wlay73IUQ](https://github.com/user-attachments/assets/296e3935-f46f-4cb6-b881-ac2a8465a3d4)


### **Decision Tree**
- A Decision Tree is a flowchart-like structure used to make decisions based on different conditions.
- It splits the data into smaller parts, asking a series of yes/no questions about the features (input variables) to reach a decision at the end (output value).
- Each split is designed to reduce the error in the prediction, which means it helps the model get more accurate.

**Example:** Suppose you're predicting house prices based on the number of bedrooms. The decision tree might first ask if a house has more than 3 bedrooms, and based on that answer, it splits the data into two branches, each making further splits.

### **Random Forest**
A Random Forest is a collection (or "forest") of multiple decision trees working together to make more accurate predictions. In Random Forest regression, it combines the results of these decision trees to get a final prediction. Here's how it works:

- **Building Multiple Trees:** Instead of creating one big decision tree, the algorithm creates many smaller trees (hundreds or even thousands).
- **Random Subsets of Data:** Each tree is trained on a different random subset of the data. This means each tree gets to see a slightly different version of the dataset.
- **Random Features:** When each tree is being built, it only looks at a random selection of features at each split. This forces the trees to become more diverse.

### **How Does Random Forest Regression Work?**
The process of Random Forest regression involves the following steps:

1. **Data Bootstrapping (Random Sampling):**
   - The algorithm creates multiple decision trees by using different random samples of the original dataset.
   - This sampling is called "bootstrapping" and helps each tree become a little different from the others.

2. **Training the Trees:**
   - Each decision tree is trained on its respective sample data, learning to make predictions using that data.
   - During training, the algorithm also randomly selects a subset of features to consider when making splits in each tree.

3. **Making Predictions:**
   - Once all the trees are trained, we use each tree to make a prediction for the new data point.
   - In regression, the final output is calculated by averaging the predictions from all the individual trees.

### **Why Use Random Forest?**
Random Forest has several advantages over a single decision tree:

- **Reduces Overfitting:** Single decision trees can often overfit the data (memorizing the training set instead of learning patterns). Random Forest reduces this problem because it's using multiple trees trained on different subsets.
- **More Accurate:** It generally produces more accurate and reliable predictions because it takes the average of many trees, which balances out errors.
- **Robust to Noise:** It works well even if some of the data has errors or outliers.

### **Hyperparameters of Random Forest Regression**
Random Forest has several hyperparameters (settings you can adjust) that can affect its performance:

- **Number of Trees (n_estimators):** How many trees you want in your forest. More trees usually lead to better performance but require more computational power.
- **Max Depth:** The maximum depth of each tree. Limiting depth helps prevent the trees from becoming too complex and overfitting.
- **Min Samples Split:** The minimum number of data points required to split a node. Higher values prevent the tree from growing too deep.
- **Random State:** A number used to ensure that results are reproducible when the model is trained multiple times.

### **Pros and Cons of Random Forest Regression**
**Pros:**
- **High Accuracy:** It usually gives highly accurate predictions compared to simpler models.
- **Less Prone to Overfitting:** The randomness helps prevent the trees from learning too specific patterns in the data.
- **Feature Importance:** Random Forests can tell you which features are the most important for making predictions.

**Cons:**
- **Computationally Intensive:** It can be slow and memory-intensive if you have a large number of trees or a large dataset.
- **Interpretability:** While individual decision trees are easy to interpret, Random Forests are more like a "black box," making it harder to understand how predictions are made.

### **How Random Forest Handles Bias and Variance**
- **Bias:** Bias refers to the error due to overly simplistic models that cannot capture the underlying patterns. Random Forest reduces bias because it combines multiple trees.
- **Variance:** Variance is the error due to models being too sensitive to small fluctuations in the training data. By averaging the predictions from many trees, Random Forest reduces the variance.

### **Summary**
- **Random Forest regression** is an ensemble learning method that builds multiple decision trees and combines their predictions.
- It reduces problems like **overfitting** and generally leads to **more accurate** results.
- It is great for **complex datasets** with many features but can be computationally demanding.

### Implementation
![image](https://github.com/user-attachments/assets/e9a53ff7-6192-4af5-b748-e5958d70e83d)
![image](https://github.com/user-attachments/assets/b648fd73-62e8-4b01-9ac0-b96f4d481c12)

### Comparison of All Regression Models
![image](https://github.com/user-attachments/assets/e5a1697b-18ae-4ebb-8f2c-e60cf7b1971d)

### Key Takeaways 🔑
- Random Forest excels in handling complex, non-linear relationships
- Ensemble methods (like Random Forest) often outperform single models
- Model choice depends on domain, data characteristics and interpretability requirements, and moree...


___
## Day 20
### Topic: Understanding Logistic Regression - Binary Classification
*Date: October 22, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Logistic Regression
- Binary Classification concepts
- Sigmoid Function and Decision Boundary

### Detailed Notes 📝

Logistic regression is a **classification algorithm** used in machine learning to predict the probability of an outcome that has **two possible classes** (binary classification). For example, it can predict whether:
- A mobile message is **spam** or **not spam**.
- A customer will **buy** or **not buy** a product online.
- A patient has **diabetes** or **does not**.

Unlike **linear regression**, which predicts continuous values (like predicting income), logistic regression predicts **probabilities** that an event will happen or not. Based on this probability, we classify the data into one of two categories.

### Key Concepts of Logistic Regression

1. **Sigmoid Function**:
   - Logistic regression uses the **sigmoid function** to map any real-valued number into a value between 0 and 1.
   - The sigmoid function formula is:
     \[
     \text{Sigmoid}(z) = \frac{1}{1 + e^{-z}}
     \]
     Here, \( z \) is the output from a linear combination of input features (like in linear regression). The sigmoid function ensures that the predicted probability is always between 0 and 1.
   - If the sigmoid function outputs a value close to 1, the model is more confident that the outcome is in the **positive class** (e.g., spam, buy, diabetes). If it’s close to 0, the model predicts the **negative class**.

2. **Decision Boundary**:
   - Once the logistic regression model calculates a probability, it needs to decide whether to classify the input into the positive class or the negative class.
   - Typically, we use a **threshold** of 0.5:
     - If the probability is **>= 0.5**, we predict the positive class.
     - If the probability is **< 0.5**, we predict the negative class.
   - This threshold can be adjusted based on the problem.

![image](https://github.com/user-attachments/assets/bf3fb475-e103-4086-bbbd-29a28b91263e)
![image](https://github.com/user-attachments/assets/c7cc1f38-3fa2-4fb8-b8f3-9f3d3a8254a7)


### How Logistic Regression Works

1. **Training the Model**:
   - Logistic regression works by finding a relationship between the **input features** (independent variables) and the **target variable** (the binary outcome).
   - During training, it uses a method called **maximum likelihood estimation** to find the best coefficients (weights) for the input features. These weights help the model to correctly classify the training data.

2. **Model Equation**:
   The model predicts the probability using the sigmoid function. The linear equation for logistic regression looks like this:
   \[
   z = w_0 + w_1x_1 + w_2x_2 + .... + w_nx_n
   \]

   Here:
   - \( w_0 \) is the **intercept** (bias).
   - \( w_1, w_2, ...., w_n \) are the **weights** (coefficients) for the features \( x_1, x_2, ...., x_n \).

   This linear equation is then passed through the sigmoid function to give us the predicted probability of the outcome being positive.

### Example: Predicting if a Student Will Pass SEE Exam

Let’s say you have data about students’ study hours and whether they passed their **SEE (Secondary Education Examination)** (yes or no). Logistic regression would try to learn the relationship between the number of study hours and the probability of passing the exam.

- Input (features): Study hours.
- Output (target): Whether the student passed (1) or failed (0).

The model learns how much weight (importance) to assign to study hours. It then uses the sigmoid function to give a probability, such as:
- If the probability is 0.8, we predict that the student **passes** (since 0.8 > 0.5).
- If the probability is 0.4, we predict that the student **fails** (since 0.4 < 0.5).

### Strengths and Weaknesses of Logistic Regression

**Strengths**:
- **Simple and interpretable**: Easy to understand how the model works and how each feature impacts the outcome.
- **Efficient for binary classification**: Performs well when there’s a clear separation between two classes.
- **Probabilistic output**: Provides a probability for predictions, which can be useful in decision-making.

**Weaknesses**:
- **Limited to linear relationships**: It assumes a linear relationship between the input features and the log-odds of the outcome.
- **Not effective for non-linear problems**: If the data cannot be separated by a straight line, logistic regression may struggle.
- **Overfitting**: If there are too many irrelevant features, the model may overfit, meaning it performs well on training data but poorly on unseen data.

Logistic regression is one of the simplest yet most effective algorithms for solving binary classification problems. It predicts the probability of an event happening and is widely used in various fields, including health (predicting diseases), marketing (customer behavior), and education (exam predictions). While it has its limitations, it’s a great starting point for understanding more complex classification models.

### Key Takeaways 🔑
- Logistic regression transforms linear combinations into probabilities
- Uses sigmoid function to bound outputs between 0 and 1
- Decision boundary determines final classification
- Ideal for binary classification problems
- Provides probabilistic interpretation of predictions


___
## Day 21
### Topic: Cost Function and Gradient Descent in Logistic Regression
*Date: October 23, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Cost Function for Logistic Regression
- Log Loss Cost Function
- Gradient Descent Optimization for choosing best params for Logistic Regression Model

### Detailed Notes 📝

When training a logistic regression model, we need a way to evaluate how well the model's parameters (weights, \( w \), and bias, \( b \)) fit the training data. This is where the **cost function** comes in. The cost function provides a way to measure the error of the model's predictions and helps us adjust the model's parameters to improve its performance.

### Why Not Use Squared Error Cost Function for Logistic Regression?

In linear regression, we typically use the **squared error cost function**, which works well because the relationship between the features and the target is linear. However, in **logistic regression**, which is used for **binary classification** (where the target variable \( y \) can only be 0 or 1), this cost function doesn't work well. Here's why:

- Logistic regression predicts probabilities that lie between 0 and 1, using the **sigmoid function** \( f(x) = \frac{1}{1 + e^{-(wx + b)}} \).
- If we apply the squared error cost function, the resulting **cost surface** (a plot of the cost values for different parameters) is **non-convex**. This means the cost surface has many local minima (dips), which makes it difficult for **gradient descent** to find the **global minimum**.

### The Logistic Regression Cost Function

![image](https://github.com/user-attachments/assets/fe2b6560-a2e7-4274-beba-e67c8998a4ba)


To ensure a smooth, convex cost surface (which guarantees convergence to the global minimum), we use a different cost function for logistic regression. Here's how it works:

1. **Loss Function for a Single Example**: We define the **loss function** to measure how well the model predicts a single training example. The loss depends on whether the true label \( y \) is 1 or 0:
   - If \( y = 1 \): the loss is -log(f(x)) where f(x) is the predicted probability.
   - If \( y = 0 \): the loss is -log(1 - f(x)), where 1 - f(x) is the predicted probability of the label being 0.

   The idea is that if the model's prediction is **close to the true label**, the loss will be small. But if the prediction is far from the true label, the loss will be large. For example:
   - If \( y = 1 \) and the model predicts a probability close to 1, the loss is close to 0.
   - If \( y = 1 \) but the model predicts a probability close to 0, the loss becomes very large.

   Similarly:
   - If \( y = 0 \) and the model predicts a probability close to 0, the loss is close to 0.
   - If \( y = 0 \) but the model predicts a probability close to 1, the loss becomes large.

2. **Cost Function for the Entire Training Set**: The **cost function** aggregates the losses over all training examples to measure how well the model is doing overall:
   \
   J(w, b) = 1/m ∑ L(f(x), y)
   \

   This cost function is **convex**, meaning that gradient descent can reliably find the global minimum, ensuring that we find the best parameters.

3. **Simplified Cost Function**:
![image](https://github.com/user-attachments/assets/cffd2b97-efb1-41bf-bcbd-5847f86152e9)


### Gradient Descent for Logistic Regression

Now that we have a proper cost function, the goal is to find the parameters \( w \) and \( b \) that minimize the cost function, i.e., the parameters that make the model's predictions as accurate as possible.

**Gradient Descent** is the algorithm used for this. Here's how it works:

1. Parameter Initialization:
   - Start with random or zero values for w and b

2. Iterative Updates:
   ```
   w := w - α ∂J(w,b)/∂w
   b := b - α ∂J(w,b)/∂b
   ```
   where α = learning rate

3. Convergence:
   - Continue updates until cost function stabilizes
   - Parameters reach optimal values for classification

![image](https://github.com/user-attachments/assets/e8104542-8997-43ab-9fd4-6a4e7b2433bf)

### Optimization Characteristics
- Cost function is convex (bowl-shaped)
- Guaranteed to find global minimum
- Learning rate α controls step size:
  - Too large: May overshoot
  - Too small: Slow convergence

### Summary

- **Cost function**: Measures how well the model's parameters fit the data. For logistic regression, we use a specific cost function derived from the log-loss, ensuring a smooth, convex surface.
- **Gradient descent**: An iterative algorithm that adjusts the model's parameters by following the negative gradient of the cost function until it finds the parameters that minimize the cost.

This process helps the logistic regression model learn the best values for \( w \) and \( b \), so it can make accurate predictions on new data.

### Key Takeaways 🔑
- Logistic regression needs special cost function (log loss)
- Cost function measures prediction accuracy
- Gradient descent finds optimal parameters
- Convex surface ensures convergence
- Learning rate crucial for optimization
- Process iteratively improves model performance


___
## Day 22
### Topic: Deepdive and Implementation of Logistic Regression
*Date: October 24, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding Maximum Likelihood Estimation (MLE)
- Regularization in Logistic Regression
- Practical Implementation with sklearn
- Real-world Applications and Evaluation Metrics, Cross-validation Techniques

### Detailed Notes 📝

### Maximum Likelihood and Regularized Logistic Regression

In logistic regression, the model estimates probabilities using the sigmoid function, and the process of estimating the model parameters (`w` and `b`) can be viewed as a Maximum Likelihood Estimation (MLE) problem. The goal is to find the parameters that maximize the likelihood of the observed data given the model. The cost function, also known as the **log-likelihood** function, for logistic regression is minimized to achieve this.

![image](https://github.com/user-attachments/assets/21301db0-b296-4a2b-9da6-686515150196)


However, when dealing with many features or complex models (like high-order polynomials), logistic regression can suffer from **overfitting**, which means it performs well on training data but poorly on unseen data. To address this, **regularization** is introduced.

**Regularized Logistic Regression** adds a penalty to the cost function to discourage overly complex models. The modified cost function becomes:

J(w, b) = existing cost function + λ/2m ∑(wⱼ²)

The second term, is the **regularization term** (L2 regularization) that penalizes large weights, which helps prevent overfitting by keeping the weights small. The gradient descent update rule is modified to account for this term, ensuring that the parameters are updated in a way that balances both fitting the data well and avoiding complex decision boundaries.

![image](https://github.com/user-attachments/assets/66c73132-a0d4-4bfb-bcdc-d224b63d7a7b)


### Python Implementation of Logistic Regression on the Iris Dataset

![image](https://github.com/user-attachments/assets/a3dd7133-673e-4230-a089-b432b43db976)


- **Dataset**: The Iris dataset was used, but for simplicity, I performed **binary classification** by removing one class (i.e., focusing on just two classes).
- **Model**: I used the `LogisticRegression` model from `sklearn` and trained it on the training set.
- **Evaluation**: I evaluated the model using metrics like **accuracy**, **precision**, **recall**, **F1 score**, and the **confusion matrix**.
- **Key Result**: The predictions and the corresponding performance metrics (accuracy, precision, recall, and F1 score) help determine how well the model performs on the test data.

### Logistic Regression on the Breast Cancer Dataset (Real-World Example)

![image](https://github.com/user-attachments/assets/810429ae-2729-4575-9c8d-0aebae1e3aa1)


- **Dataset**: The breast cancer dataset has multiple features (over 9), and I tried to classify whether a tumor is benign or malignant.
- **Model**: I used `LogisticRegression` from `sklearn` to fit the model on the training data.
- **Confusion Matrix**: A confusion matrix was printed to evaluate how many true positives, false positives, true negatives, and false negatives were predicted by the model.
- **Cross-Validation**: I applied **10-fold cross-validation** to get a better estimate of the model’s performance. This is useful because it gives insight into how the model generalizes across different splits of the dataset, and also provides an estimate of performance variability (standard deviation).

Both implementations of **Logistic Regression** in Python show how the algorithm can be applied to real-world problems, like binary classification in the Iris dataset and breast cancer classification. Regularization, if implemented, would have helped avoid overfitting, especially in the breast cancer dataset with many features. Through metrics like **accuracy**, **precision**, **recall**, and **cross-validation**, I’ve ensured that the model performs well on unseen data.


___
## Day 23
### Topic: KNN, Cross-Validation, Hyperparameter Tuning, Model Evaluation Techniques, Pipelining
*Date: October 25, 2024*

**Today's Learning Objectives Completed ✅**
- Understanding K-Nearest Neighbors (KNN) Algorithm
- Cross-Validation Techniques
- Classification & Regression Evaluation Metrics
- Hyperparameter Tuning Methods
- Pipeline Construction

### Detailed Notes 📝

### K-Nearest Neighbors (KNN)
K-Nearest Neighbors (KNN) is one of the simplest and most intuitive machine learning algorithms. It's a **supervised learning algorithm** that can be used for **classification** tasks. Here's how it works:

#### How KNN works:
- The algorithm doesn't actually "learn" anything during training. Instead, it stores the training data and makes decisions by looking at the data during prediction.
- For a new data point that we want to classify or predict, KNN finds the **K nearest points** in the training set.
- These "nearest points" are found based on a distance metric like **Euclidean distance** (the straight-line distance between two points in space).
 - It checks the **class labels** of the K closest data points and assigns the most common class label (majority vote) to the new point.

#### Example:
Imagine you're trying to classify a fruit as either an apple or a banana. You collect some features like color and size, and plot them on a graph. Now, when you encounter a new fruit that you want to classify, KNN looks at the closest data points (fruits) on the graph and classifies your new fruit based on its "neighbors."

#### Choosing K:
- A **small K** (e.g., K = 1) can lead to **overfitting**, where the model is too sensitive to noise in the training data.
- A **large K** (e.g., K = 50) can lead to **underfitting**, where the model is too simplistic and doesn't capture enough details from the data.

![image](https://github.com/user-attachments/assets/12c7c7d7-715a-4ea0-92da-005dbd2959a0)

---

### **Cross-Validation**
Cross-validation is a technique used to evaluate the performance of a machine learning model. The idea is to avoid overfitting and ensure the model generalizes well to unseen data.

#### How Cross-Validation Works:
- The most common form is **K-Fold Cross-Validation**.
- Here, the dataset is split into K equal-sized parts, called "folds".
- The model is trained on K-1 of the folds and tested on the remaining fold.
- This process is repeated K times, with each fold being used as the test set once.
- Finally, the performance (accuracy, precision, etc.) is averaged over all K runs to get a better estimate of the model's general performance.

#### Example (K-Fold Cross-Validation):
If you have a dataset of 100 samples and use 5-fold cross-validation, the data is divided into 5 parts of 20 samples each. The model trains on 80 samples and tests on 20 samples, repeating this 5 times so each fold acts as the test set once. Then you average the performance metrics (like accuracy) across all 5 runs.

---

### Evaluation Metrics for Classification

When evaluating classification models, a range of metrics assess how well the model distinguishes between classes. Let’s discuss the key metrics in depth:

1. **Confusion Matrix**:
   - **Definition**: A table showing counts for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).
   - **Example**: For a model that predicts if an email is spam:
     - **TP**: Model correctly identifies a spam email.
     - **TN**: Model correctly identifies a non-spam email.
     - **FP**: Model incorrectly labels a non-spam email as spam.
     - **FN**: Model incorrectly labels a spam email as non-spam.


   |               | Predicted Spam | Predicted Not Spam |
   |---------------|----------------|--------------------|
   | **Actual Spam**     | TP             | FN                 |
   | **Actual Not Spam** | FP             | TN                 |

2. **Accuracy**:
   - **Formula**: (TP + TN)/(TP + TN + FP + FN)

   - **Explanation**: Measures the proportion of correct predictions. It’s best used when the classes are balanced, as it can be misleading in imbalanced datasets.

3. **Precision**:
   - **Formula**: TP/(TP + FP)
   - **Explanation**: Precision represents how many of the positive predictions are truly positive. High precision means fewer false positives.
   - **Example**: In spam detection, a high precision means most emails flagged as spam are actually spam.

4. **Recall**:
   - **Formula**: TP/(TP + FN)
   - **Explanation**: Recall (also known as sensitivity or true positive rate) measures the ability to capture all actual positives. High recall means fewer false negatives.
   - **Example**: In spam detection, high recall means that most spam emails are detected as spam.

5. **F1 Score**:
   - **Formula**: 2 × (Precision × Recall)/(Precision + Recall)
   - **Explanation**: The F1 Score is a harmonic mean of precision and recall, balancing both metrics. It’s especially useful in cases of imbalanced data where precision and recall are both important.
   - **Example**: If a spam detector has high recall but low precision, the F1 Score will indicate a balanced view of its performance.

6. **ROC Curve and AUC (for Binary Classification)**:
   - **ROC Curve**: Plots the **True Positive Rate** (Recall) against the **False Positive Rate** across different threshold values.
   - **AUC (Area Under Curve)**: Represents the model’s capability to rank positive samples higher than negative ones. A higher AUC (close to 1) indicates better performance.
   - **Example**: In medical diagnosis, an AUC of 0.9 means the model is 90% likely to rank a positive case higher than a negative one.

---

### **Evaluation Metrics for Regression**

In regression problems, metrics quantify the difference between the predicted and actual values.

1. **Mean Absolute Error (MAE)**: Measures the average absolute difference between predicted and actual values, making it easy to interpret in the same units as the target variable.

2. **Mean Squared Error (MSE)**: Penalizes larger errors more heavily by squaring them, which can be useful if large errors are particularly undesirable.

3. **Root Mean Squared Error (RMSE)**: Often easier to interpret than MSE, RMSE has the same units as the target variable and is useful for assessing the standard deviation of errors.

4. **R-squared (R²)**: Represents the proportion of variance explained by the model, where a score closer to 1 indicates a better fit.

---

### **Hyperparameter Tuning with GridSearchCV and RandomizedSearchCV**

Hyperparameter tuning involves optimizing settings that are not learned from data (like `k` in KNN, max depth in decision trees). Two popular methods for tuning:

- **GridSearchCV:** Exhaustively tests a range of values for each hyperparameter, creating every possible combination and training the model on each.
- **RandomizedSearchCV:** Randomly selects combinations of hyperparameters for testing, which is faster and often effective for large search spaces.

---

### **Using `cross_val_score` to Analyze Model Performance**

`cross_val_score` allows you to run cross-validation on different models to assess which performs best on your dataset. It takes in a model and returns the performance score for each fold, which can then be averaged to evaluate stability and effectiveness across folds.

---

### **Pipelines for Efficient Workflow**

A pipeline lets you streamline data preprocessing (like imputation and scaling) and model training in a single, organized workflow.

**Example:** For a classification task, you can create a pipeline to:
1. Fill in missing values with an imputer.
2. Scale features using StandardScaler.
3. Pass the data into a KNN or Logistic Regression model.

Each of these steps (imputing, scaling, and model selection) can include hyperparameter tuning within the pipeline itself, making the entire process efficient and less error-prone.

### Implementation
**Simple KNN with pipelines, and hyperparameter tuning**

![image](https://github.com/user-attachments/assets/54d7fcce-4526-426c-926e-c86af6ce274a)

**Classification model with analysing accuracy scores for different models**

![image](https://github.com/user-attachments/assets/3bb860ff-3dc5-4551-9bbe-10eab9435cf4)
![image](https://github.com/user-attachments/assets/1410f048-0ebd-4edf-9c69-c2fb0c4e05aa)


___
## Day 24
### Topic: Support Vector Machines
*Date: October 26, 2024*

### Detailed Notes 📝

Support Vector Machines (SVMs) are a type of supervised machine learning algorithm used for classification and regression tasks. They are widely used in various fields, including pattern recognition, image analysis, and natural language processing.

SVMs work by finding the optimal hyperplane that separates data points into different classes.

The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.

![image](https://github.com/user-attachments/assets/7927b25d-e51a-46cb-af31-769c525556bf)

![1_oRk-5aab0G8SkBX2fpw8Gw](https://github.com/user-attachments/assets/09fe8813-38f8-4c0b-b756-8d1d30170611)


---

### **Intuition Behind Support Vector Machines**

Imagine we have two classes of data: for example, pictures of cats and dogs. Each picture has unique features (like color, size, etc.), which we can represent in a graph or a coordinate plane. We want to separate these two classes in a way that if a new picture comes in, we can tell whether it’s a cat or a dog just based on where it falls on this graph.

### Goal of SVM:
SVM finds the best "boundary line" (or "hyperplane" in higher dimensions) that separates these classes with the largest possible "margin." Think of this boundary as a line or plane that tries to keep the two classes as far apart as possible to minimize misclassification.

To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.

---

### **Key Terminologies in SVM**

- **Hyperplane:** This is the line or plane that separates different classes. In 2D, it’s a line; in 3D, it’s a plane; in higher dimensions, it’s a “hyperplane.”

  ![1_ZpkLQf2FNfzfH4HXeMw4MQ](https://github.com/user-attachments/assets/cde8c0ca-a26f-49c2-ab65-db73362a803b)


- **Margin:** The distance between the hyperplane and the nearest points from either class. SVM tries to maximize this distance, giving us the best separation between classes.

- **Support Vectors:** These are the data points closest to the hyperplane. They "support" the position of the hyperplane. The margin is based on the distance between the support vectors and the hyperplane.

- **Soft Margin vs. Hard Margin:**
  - **Hard Margin SVM** requires that all points be correctly classified with a strict boundary.
  - **Soft Margin SVM** allows for some misclassification by introducing a “slack” variable, making it more robust for overlapping classes.

- **Kernel Trick:** In real-world data, classes aren’t always linearly separable. Kernels allow SVM to work in non-linear spaces by projecting the data into higher dimensions where it can be separated with a hyperplane.

---

### **Core Concept: Finding the Optimal Hyperplane**

![0_ecA4Ls8kBYSM5nza](https://github.com/user-attachments/assets/3812ac63-9753-4d66-a5fc-2f90d9daf182)


The key to SVM is finding the "optimal" hyperplane, which maximizes the margin. Here’s how we approach it step-by-step:

1. **Place a Boundary Line**: Start by placing a line to separate the classes, such as between cats and dogs. But we don’t want just any line; we want the one that maximizes the margin.

2. **Maximize the Margin**: We adjust the line’s position to maximize the distance between it and the nearest points of each class. This helps SVM be more confident in its classification.

3. **Use Support Vectors**: Only the closest data points (the support vectors) influence the position of this optimal hyperplane. Any points further away from the margin don’t affect it.

4. **Minimize Misclassification with a Soft Margin (if necessary)**: If some points overlap or are noisy, we allow some errors by introducing a soft margin to avoid overfitting.

---

### **Examples of SVM in Action**

![image](https://github.com/user-attachments/assets/6c07449c-1d67-4e69-9a4b-161751c1d197)


Let’s look at a couple of scenarios:

#### Simple Case (Linear Separability):
Imagine two sets of points on a 2D plane: red circles and blue squares. If we can draw a straight line to separate them without any overlap, SVM finds that line with the largest margin.

#### Complex Case (Non-linear Separability):
Now imagine if the circles and squares are mixed in a way that no straight line can separate them. This is where the **Kernel Trick** comes in.

### **Kernel Trick in Detail:**

![image](https://github.com/user-attachments/assets/93383d50-4d45-4b5e-8504-09aec3ad6c58)


A kernel function transforms data into a higher dimension where a hyperplane can separate it. Some common kernel types are:

1. **Linear Kernel**: For linearly separable data.
2. **Polynomial Kernel**: Allows for curved boundaries by transforming data into a polynomial space.
3. **Radial Basis Function (RBF) Kernel**: Projects data into a high-dimensional space where complex boundaries can be created.

The RBF kernel is popular because it can handle almost any shape of data.

---

### **Evaluating and Using SVM Models**

When using an SVM, we need to tune some **hyperparameters**:

1. **C (Regularization Parameter)**: Controls the trade-off between maximizing the margin and minimizing classification error. A smaller C makes a wider margin but allows for more misclassification; a larger C tries to classify everything correctly but may lead to a narrow margin (overfitting).

2. **Kernel Parameters (like Gamma in RBF)**: Adjusts how each data point influences the classification boundary. A larger gamma value makes points close to the boundary more important, while a smaller gamma smooths out the influence.

---

### **Pros and Cons of SVM**

#### **Pros:**
- **Effective in high-dimensional spaces**: Works well with many features, even if there are more features than samples.
- **Versatile with kernels**: Kernels let SVM model complex, non-linear boundaries.
- **Effective for both linear and non-linear classification**.

#### **Cons:**
- **Computationally Intensive**: SVMs can be slow for large datasets.
- **Sensitive to parameters**: Needs careful tuning of C and kernel parameters.
- **Not ideal for very noisy data**: Outliers can heavily influence the margin, making the model less robust.

---

### **Where SVM is Commonly Used**

SVM is popular for:
- **Image recognition**: Such as detecting faces, animals, or objects.
- **Text classification**: Separating spam from non-spam emails.
- **Medical diagnostics**: Classifying diseases based on genetic or health data.

---

Support Vector Machines are powerful for separating classes with a clear margin, even when the data isn’t linearly separable, thanks to the kernel trick. SVM aims to maximize the margin between classes, uses only the most “important” data points (support vectors) to find the optimal boundary, and is highly effective in high-dimensional spaces. However, SVM’s effectiveness often depends on the right choice of kernel and parameters, which makes tuning important for best results.

Support Vector Machines are powerful tools for classification tasks. The goal of SVM is to find the optimal hyperplane that separates data points of different classes by maximizing the margin between them. For non-linear data, the kernel trick enables SVM to project the data into higher dimensions, allowing linear separation in the transformed space. Key components of SVM, like the support vectors, kernel functions, and regularization, play a crucial role in creating a robust model. With its high-dimensional effectiveness, SVM has become a popular choice in applications ranging from image classification to bioinformatics.


___
## Day 25
### Topic: Implementation of Support Vector Machines
*Date: October 27, 2024*

### Detailed Notes 📝

---

Today, I successfully implemented a Support Vector Machine (SVM) classification model using the `sklearn` library to perform sentiment analysis on a tweets dataset. The dataset initially contained over 70,000 rows, but to prevent my laptop from hanging during model fitting, I limited the data to the first 10,000 entries.

This project implements a sentiment analysis system using Support Vector Machine (SVM) classification on Twitter data. The system classifies the given text into four categories: Positive, Negative, Neutral, and Irrelevant. It demonstrates the practical application of machine learning in natural language processing (NLP).

### **Steps Taken:**

1. **Data Loading and Preprocessing:**
   - Loaded the dataset using `pandas` and displayed the first few entries.
   - Renamed the columns for clarity, setting them as `ID`, `Category`, `Sentiment`, and `Text`.
   - Dropped any rows with missing text values to ensure a clean dataset.

2. **Feature and Label Extraction:**
   - Extracted the text data (`X`) and the corresponding sentiment labels (`y`) for model training and evaluation.

3. **Data Splitting:**
   - Utilized `train_test_split` to divide the dataset into training and testing sets, allocating 20% for testing.

4. **Label Encoding:**
   - Implemented `LabelEncoder` to convert sentiment labels into a numerical format suitable for classification.

5. **Text Vectorization and Model Pipeline:**
   - Created a pipeline with `TfidfVectorizer` for text feature extraction (Text Vectorization) and `SVC` with a linear kernel for classification.
   - Fitted the model to the training data.

6. **Model Testing:**
   - Conducted predictions on sample texts to verify the model's functionality:
     - "Hello what's up buddie?" → **Irrelevant**
     - "I am so happy today!" → **Positive**
     - "I am pissed off!" → **Negative**

7. **Model Evaluation:**
   - Predicted sentiment labels on the test dataset and calculated the accuracy, which was approximately **92.67%**.
   - Generated a classification report, detailing precision, recall, and F1 scores for each sentiment category.

### **Results:**
- **Accuracy:** 0.9267
- **Classification Report:**
  - Precision, Recall, F1-Score, and Support for each sentiment category were provided, showing balanced performance across classes.

![image](https://github.com/user-attachments/assets/3ac7afab-6827-4155-a2a4-7a17e7fa18e9)
![image](https://github.com/user-attachments/assets/0d109468-e5ea-4ef2-a152-48c6cd86b63d)
![image](https://github.com/user-attachments/assets/044e4581-44bc-4c0c-95e9-76680d47a280)

---

### **Reflections:**
Implementing SVM was an enriching experience, especially after diving deep into its intuition and mechanics. The effectiveness of SVM in handling classification tasks makes it one of the best classifiers available, particularly when dealing with high-dimensional data like text.

- **Understanding SVM:** I found the theoretical concepts surrounding SVM—such as the decision boundary, support vectors, and the margin—to be intellectually stimulating. This deep dive into the algorithm's workings has significantly enhanced my understanding of machine learning.
- **Practical Application:** Applying the theoretical knowledge practically solidified my learning and highlighted the importance of preprocessing, feature extraction, and model evaluation in building effective machine learning solutions.
- **Model Performance:** The impressive accuracy and detailed classification report provided confidence in the model's ability to generalize. It was rewarding to see how minor adjustments in data handling and model configuration led to substantial improvements in performance.
- **Interest in NLP:** This project ignited my enthusiasm for natural language processing (NLP). The ability to extract sentiment from tweets is not only fascinating but also has practical implications in fields such as marketing, social media analysis, and public opinion research.


___
## Day 26
### Topic: Implementation of Image Classification for Brain Tumor Detection
*Date: October 28, 2024*

### Detailed Notes 📝

Today, I implemented an image classification model aimed at detecting brain tumors using a dataset of MRI-scanned images. The project involved data preprocessing, training various classifiers, and evaluating their performance, ultimately leading to the selection of Support Vector Machine (SVM) with an `rbf` kernel due to its superior accuracy in cross-validation tests.

**Steps Taken:**

1. **Data Loading and Exploration:**
   - Loaded the dataset containing file names of MRI images and their corresponding classes (0 for non-tumor and 1 for tumor) using `pandas`.
   - Displayed the first few entries of the dataset to confirm its structure and contents.

**[Dataset](https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor/) Overview**:

![image](https://github.com/user-attachments/assets/abe304c7-7794-47b9-9a1b-79af2a9046e4)


2. **Image Preprocessing:**
   - Defined a function `preprocess_image` to read and preprocess images:
     - Read images as grayscale for consistency.
     - Resized each image to 28x28 pixels, flattening the array for input into the model.
   - Applied this function to the dataset, generating feature vectors (`X`) from the image files.

3. **Data Splitting:**
   - Used `train_test_split` to divide the dataset into training and testing sets, allocating 20% for testing. This ensured that the model could be evaluated on unseen data, providing insights into its performance.

4. **Model Selection and Training:**
   - Prepared to evaluate multiple classification models, including **Logistic Regression**, **K-Nearest Neighbors (KNN)**, and **SVM** with an `rbf` kernel (as, till now, I have only learnt these 3 classification algorithms).
   - Implemented 10-fold cross-validation using to assess the models’ performance reliably.

5. **Cross-Validation and Visualization:**
   - Trained each model pipeline (including standard scaling and the classifier) and recorded cross-validation scores.
   - Created a box plot to visualize the performance of each model, revealing that SVM outperformed the others.

6. **Final Model Training:**
   - Based on the cross-validation results, I selected the SVM model with an `rbf` kernel and trained it using the training dataset.

7. **Prediction on Test Images:**
   - Tested the trained model on two sample MRI images:
     - An image with a brain tumor.
     - A non-tumor image.
   - Displayed the images and printed the prediction results, confirming the model's ability to classify the images accurately.

8. **Model Evaluation:**
   - Made predictions on the test set and evaluated the model using accuracy, classification report, and confusion matrix metrics.
   - The accuracy was around **93.3%**, indicating the model's effectiveness in distinguishing between tumor and non-tumor images.

---

**Results:**
- **Accuracy:** 0.9336
- **Classification Report:**
  - The report highlighted the precision, recall, and F1 scores for each class, indicating balanced performance:
    - Non-Tumor (0): Precision of 0.93, Recall of 0.96
    - Tumor (1): Precision of 0.94, Recall of 0.90
  - The overall metrics showed the model's reliability in both classes.

- **Confusion Matrix:**
  - Confusion Matrix displayed the true positives, true negatives, false positives, and false negatives:
    ```
    [[412  19]
     [ 31 291]]
    ```
  - The matrix suggested that while the model performed well, there were some misclassifications, particularly in the false negatives (acceptable, I guess, as we are using SVM that utilizes soft margins for misclassifications).

![image](https://github.com/user-attachments/assets/431cd1f9-edad-49a2-8b4e-aedb9a00bda5)
![image](https://github.com/user-attachments/assets/2ceb4701-f17a-438a-bacb-7366e4d5e59f)
![image](https://github.com/user-attachments/assets/71e56fe9-589a-4fd5-b55e-d4f4d0073671)
![image](https://github.com/user-attachments/assets/ba188e09-0eee-48fe-bd4e-aa7039522a71)
![image](https://github.com/user-attachments/assets/36a0c472-f5ad-4545-92f0-00b1938c58e8)


---

**Reflections:**
Implementing the brain tumor detection project with MRI images was both challenging and rewarding. The process deepened my understanding of image processing and classification techniques in machine learning.

- **Understanding Image Classification:** This project enhanced my grasp of how image preprocessing affects model performance. The resizing and flattening of images were crucial for successful model input.
- **Model Evaluation:** Evaluating multiple models and their performance through cross-validation provided insights into the strengths and weaknesses of each algorithm. This reinforced the importance of model selection based on data characteristics and problem requirements.
- **SVM's Effectiveness:** The choice of SVM with an RBF kernel was validated through its superior cross-validation scores. It was fascinating to observe how kernel functions can significantly impact classification performance.
- **Kernel Trick Bammmmm!** : Today, I gained a deeper understanding of SVM, particularly how Kernel SVM operates. The kernel trick allows the model to transform the lower-dimensional feature space to a higher-dimensional space without explicitly transforming the data. Instead, it computes the dot products in the original feature space, effectively mimicking this transformation. This approach enhances efficiency in both time and space, making it a powerful technique for handling complex datasets, literally this thing is a double, triple, 100000x BAMMM!
- **Model Limitations:** While traditional models like SVM were effective in this project, I acknowledge that they may not always be the best choice compared to deep learning models like Convolutional Neural Networks (CNNs). However, for learning purposes, I opted to implement the models I have learned thus far, comparing their performance and drawing insights through visualization.
- **Practical Applications:** This project has practical implications in the medical field, where accurate and timely diagnosis is critical. The ability to automate brain tumor detection can assist medical professionals in making informed decisions.
- **Further Improvements:** I recognize that there are opportunities for improvement, such as augmenting the dataset with more images or applying advanced techniques like deep learning for potentially better performance, but currently my learning is not in that level/scope.


___
## Day 27
### Topic: Implementation of Classification Algorithms for Student Performance Evaluation
*Date: October 29, 2024*

### Detailed Notes 📝

In today's project, I focused on evaluating student performance using various metrics gathered from a dataset. The main objective was to analyze the factors affecting students' academic performance and to build predictive models to classify their grade classes based on their GPA.

**Dataset Overview:**
The dataset consists of **2392 records** with **15 features** that capture different aspects of student performance. Here are the key features included:

- **Demographics:**
  - **Age:** Ranges from 15 to 18 years.
  - **Gender:** Coded as 0 (Male) and 1 (Female).
  - **Ethnicity:** Coded into categories such as Caucasian, African American, Asian, and Other.
  - **Parental Education:** Represents the education level of parents from None to Higher.

- **Study Habits:**
  - **StudyTimeWeekly:** The number of hours students study weekly (0 to 20 hours).
  - **Absences:** Total number of absences during the school year (0 to 30).
  - **Tutoring:** Indicates whether a student has received tutoring (0: No, 1: Yes).

- **Parental Involvement and Extracurricular Activities:**
  - **Parental Support:** Levels of support provided by parents, coded from None to Very High.
  - Participation in extracurricular activities (e.g., sports, music, volunteering).

- **Academic Performance:**
  - **GPA:** Grade Point Average ranging from 2.0 to 4.0.
  - **Grade Class:** Target variable representing classification based on GPA:
    - 0: 'A' (GPA >= 3.5)
    - 1: 'B' (3.0 <= GPA < 3.5)
    - 2: 'C' (2.5 <= GPA < 3.0)
    - 3: 'D' (2.0 <= GPA < 2.5)
    - 4: 'F' (GPA < 2.0)

---

**Exploratory Data Analysis (EDA):**
I performed EDA to understand the distribution of grades and the relationships between various factors and academic performance:

1. **Grade Distribution:** A pie chart displayed the distribution of students across different grade classes.
2. **Boxplots:**
   - **GPA by Grade Class:** Analyzed how GPA varies with different grade classifications.
   - **Absences by Grade Class:** Investigated the relationship between absences and grade class.
   - **Study Time by Grade Class:** Explored the correlation between weekly study hours and academic performance.

---

**Model Building:**
Using various classification models, I aimed to predict student performance based on the features available:

1. **Data Preparation:**
   - Split the dataset into training and testing sets (70/30 split).
   - Standardized the features using `StandardScaler`.

2. **Model Selection:**
   - Evaluated several models including Decision Trees, Random Forests, SVMs, KNN, and Logistic Regression using **GridSearchCV** for hyperparameter tuning.
   - Implemented **K-Fold Cross Validation** to ensure model robustness.

3. **Results:**
   - The best model was selected based on accuracy scores from cross-validation.
   - The final model achieved an accuracy of **94.85%** on the test set, with a comprehensive classification report showcasing precision, recall, and F1 scores across different grade classes.

4. **Confusion Matrix:**
   - Displayed the confusion matrix for the best-fitted model, providing insights into the model's performance in classifying each grade.

---

**Results**

- **Accuracy**:
  - The final model achieved an accuracy of around **94.8%**, indicating strong predictive performance.

- **Classification Report**:
  The classification report displayed precision, recall, and F1 scores for each class:
  ```
                precision    recall  f1-score   support

          0.0       0.78      0.84      0.81        43
          1.0       0.88      0.89      0.88        72
          2.0       0.98      0.95      0.97       107
          3.0       0.91      0.96      0.93       112
          4.0       0.99      0.97      0.98       384

      accuracy                           0.95       718
     macro avg       0.91      0.92      0.91       718
  weighted avg       0.95      0.95      0.95       718
  ```
  This indicates balanced performance across classes, with high precision and recall for most grade classes, particularly for lower grades (2.0 and 3.0).

- **Confusion Matrix**:
  The confusion matrix illustrated the model's predictions, showing true positives, true negatives, false positives, and false negatives. It provided insights into areas of misclassification, particularly in lower-performing grade classes.

![image](https://github.com/user-attachments/assets/f209805d-7daf-45a4-b0af-3c81ba283f9d)
![image](https://github.com/user-attachments/assets/0bbd3da4-8faa-474d-8c7a-58d0119ee238)
![image](https://github.com/user-attachments/assets/75b99575-75ea-418d-b3df-2e6caee9a1de)
![image](https://github.com/user-attachments/assets/4da2dbcb-4eb7-49da-a16b-b92bc7c5cfb1)

---

**Reflections:**

- **Understanding Student Performance Factors**: This project allowed me to explore the multifaceted factors influencing student performance, reinforcing the importance of demographic and behavioral attributes in academic outcomes.

- **Model Evaluation and Selection**: Through evaluating multiple models, I gained insights into the significance of hyperparameter tuning and model selection based on data characteristics. The Decision Tree emerged as the most effective model for this dataset.

- **Data Visualization Insights**: Visualizing the data revealed critical patterns, such as the relationship between study time and grade class, which can inform educational strategies and interventions.

- **Practical Implications**: The findings from this project have practical applications in educational settings, enabling educators to identify at-risk students and develop targeted support strategies based on empirical data.

- **Future Enhancements**: Opportunities for further improvement include integrating more features, such as behavioral metrics or historical performance, and exploring advanced techniques like ensemble methods or deep learning models to enhance predictive capabilities.

Overall, this project was an enriching experience that deepened my understanding of data analysis, model evaluation, and the practical implications of different classification algorithms.


___
## Day 28
### Topic: Decision Trees, Random Forest, and Extra Trees Classification Algorithms
*Date: October 30, 2024*

### Detailed Notes 📝

Decision trees, random forests, and extra trees are widely used machine learning algorithms for classification and regression tasks. Today I got an in-depth overview of these methods, including their principles, key terms, mathematical intuition, how they work, evaluation metrics, and applications.

### Decision Tree Classification Algorithm

#### What is a Decision Tree?

A decision tree is a flowchart-like structure where:

- **Internal Nodes:** Represent tests on attributes.
- **Branches:** Indicate the outcome of these tests.
- **Leaf Nodes:** Represent class labels or final decisions.

#### How It Works

1. **Root Node Creation:** Start with the entire dataset at the root node.

2. **Splitting Criteria Selection:**
   - Use Gini impurity or entropy to evaluate potential splits.
   - Choose the feature that results in the highest information gain or lowest impurity after splitting.

3. **Recursive Partitioning:**
   - Split the dataset into subsets based on the chosen feature.
   - Repeat this process recursively for each subset until stopping criteria are met (e.g., maximum depth, minimum samples per leaf).

4. **Leaf Node Assignment:**
   - Once terminal nodes are reached, assign labels based on majority class or average value for regression tasks.


![decision_tree_for_heart_attack_prevention_2140bd762d](https://github.com/user-attachments/assets/07cb85ea-43d4-4b9b-96e9-c70cac351d57)
![2_btay8n](https://github.com/user-attachments/assets/a118f760-8bec-4c10-8f70-562741320f30)


#### Key Terms

- **Gini Impurity:** Measures how often a randomly chosen element would be incorrectly labeled. Formula:

![image](https://github.com/user-attachments/assets/7352ad5c-1a71-4ccd-836d-0b6deeb87b25)

  Where:
  - Σj represents the summation over all classes j
  - pj is the probability or proportion of instances belonging to class j

- **Entropy:** Measures disorder or randomness in the dataset. Formula:

![image](https://github.com/user-attachments/assets/9fbe8138-4fc1-49d4-b026-16473d25556a)

Where:
  - Σ_j represents the summation over all classes j
  - p_j is the probability or proportion of instances belonging to class j
  - log_2(p_j) is the logarithm of p_j to the base 2

- **Information Gain:** Reduction in entropy or impurity after a dataset is split. Formula:

  ![image](https://github.com/user-attachments/assets/a9e13d42-2bbd-46df-855c-e3a87b226f47)

Where:
  - Entropy(parent) is the entropy of the parent node before the split
  - Entropy(left) and Entropy(right) are the entropies of the left and right child nodes after the split
  - N_left and N_right are the number of instances in the left and right child nodes
  - N is the total number of instances in the parent node

- **Overfitting:** When a model learns noise from training data, negatively impacting performance on new data.

- **Pruning:** The process of removing sections of a decision tree to reduce complexity and overfitting.

#### Pros and Cons

**Pros:**

- Simple to understand and interpret.
- Requires little data preprocessing (no need for scaling).
- Can handle both numerical and categorical data.

**Cons:**

- Prone to overfitting, especially with complex trees.
- Sensitive to small changes in data.
- Decision boundaries are axis-aligned, limiting flexibility.

#### Applications

- Customer segmentation.
- Medical diagnosis (e.g., predicting disease based on symptoms).
- Credit scoring.

#### Conclusion

Decision tree classification is powerful algorithm in machine learning. Decision trees offer simplicity and interpretability, making them suitable for quick insights.

---

### Random Forest Classification Algorithm
Random Forest Classification is an ensemble learning method that builds multiple decision trees and merges their predictions to improve accuracy and robustness. This document provides an in-depth overview of the Random Forest Classification algorithm, including its principles, how it works, key terms, mathematical intuition, advantages, disadvantages, applications, and evaluation metrics.

#### What is Random Forest Classification?

Random Forest Classification is an ensemble method that utilizes a multitude of decision trees to make predictions. Each tree in the forest votes for a class label, and the class with the majority vote becomes the final prediction. This approach helps to mitigate overfitting and enhances model performance.

#### How Random Forest Classification Works

1. **Bagging:**
   - Bagging: Random forest uses bootstrap aggregating (bagging) to create multiple decision tree models from random subsets of the training data. This reduces overfitting and improves generalization.
   - Randomly sample subsets of the training data with replacement to create multiple datasets. Each dataset is used to train a separate decision tree.

3. **Tree Construction:**
   - For each bootstrap sample, a decision tree is built using a random subset of features at each node. This randomness helps create diverse trees that capture different patterns in the data.

4. **Prediction Aggregation:**
   - For classification tasks, predictions from all trees are aggregated using majority voting. The class that receives the most votes across all trees is selected as the final prediction.


![random-forest-classifier-3](https://github.com/user-attachments/assets/24147d56-4200-47dc-89e6-b607bfd340cd)



#### Key Terms

- **Ensemble Learning:** A machine learning paradigm where multiple models (like decision trees) are combined to produce better predictive performance than individual models.

- **Bootstrap Sampling:** A technique where samples are drawn with replacement from the training dataset to create multiple training sets for each tree.

- **Feature Subset Selection:** The process of selecting a random subset of features for each split in the decision tree, which adds diversity among trees.

- **Gini Impurity:** A measure of how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.
  - Formula:

    ![image](https://github.com/user-attachments/assets/7352ad5c-1a71-4ccd-836d-0b6deeb87b25)

- **Entropy:** A measure of disorder or randomness in the dataset.
  - Formula:

    ![image](https://github.com/user-attachments/assets/9fbe8138-4fc1-49d4-b026-16473d25556a)

- **Information Gain:** The reduction in entropy or impurity after a dataset is split.
  - Formula:

    ![image](https://github.com/user-attachments/assets/a9e13d42-2bbd-46df-855c-e3a87b226f47)

#### Advantages of Random Forest Classification

- **High Accuracy:** Generally provides better accuracy than individual decision trees due to averaging predictions from multiple trees.

- **Robustness:** Less prone to overfitting compared to single decision trees due to its ensemble nature.

- **Handles Missing Values:** Can maintain accuracy even when some data points have missing values.

- **Feature Importance:** Provides insights into feature importance, helping identify which features contribute most to predictions.

#### Disadvantages of Random Forest Classification

- **Complexity:** More complex than single decision trees, making it harder to interpret individual tree behavior.

- **Training Time:** Requires more computational resources and time for training due to building multiple trees.

- **Memory Usage:** Can consume significant memory when dealing with large datasets due to storing multiple trees.

#### Applications

- **Medical Diagnosis:** Predicting diseases based on patient symptoms and medical history.

- **Credit Scoring:** Assessing credit risk by analyzing customer data.

- **Customer Segmentation:** Grouping customers based on purchasing behavior for targeted marketing strategies.

- **Fraud Detection:** Identifying fraudulent transactions in banking and finance sectors.

#### Evaluation Metrics

To evaluate the performance of Random Forest Classification, several metrics can be used:

- **Accuracy:** The proportion of true results among total cases.

- **Precision and Recall:** Important for imbalanced datasets; precision measures correctness among positive predictions while recall measures how well positive instances are captured.

- **F1 Score:** The harmonic mean of precision and recall, providing a balance between them.

- **Area Under Curve (AUC):** Evaluates model performance across all classification thresholds; higher values indicate better performance.

- **Confusion Matrix:** A table that summarizes the performance of a classification algorithm by showing true positives, false positives, true negatives, and false negatives.

#### Mathematical Intuition

Random Forest uses decision trees as base learners and combines their outputs through majority voting. By introducing randomness in both data sampling (bootstrap sampling) and feature selection at each split, Random Forest reduces variance without significantly increasing bias. This balance enhances generalization on unseen data while maintaining high accuracy.

#### Conclusion

Random Forest Classification is a powerful ensemble method that leverages multiple decision trees to improve predictive performance while reducing overfitting risks. Its ability to handle complex datasets with missing values makes it suitable for various applications across different domains. Understanding its workings, advantages, disadvantages, evaluation metrics, and mathematical principles allows practitioners to effectively implement this algorithm in machine learning projects.

---

### Extra Trees Classification Algorithm

The Extra Trees, or Extremely Randomized Trees, is an ensemble learning method that builds upon the principles of decision trees and random forests. Basically it is more random than Random Forest itself.

#### What is Extra Tree?

Extra Trees is a tree-based ensemble learning algorithm that constructs multiple decision trees and aggregates their predictions to improve accuracy and robustness. It is particularly known for its speed and efficiency compared to traditional random forests.

#### How Extra Trees Works

1. **Data Sampling:**
   - Unlike Random Forests that use bootstrapping (sampling with replacement), Extra Trees uses the entire dataset to train each tree. However, it selects a random subset of features for splitting.

2. **Feature Selection:**
   - For each node in the tree, a random subset of features is selected (usually the square root of the total number of features). Instead of calculating the optimal split point based on Gini impurity or entropy, a split value is chosen randomly from the selected feature.

3. **Tree Construction:**
   - Each tree is built independently using the entire dataset and the randomly selected splits. This results in more diverse trees compared to those built in Random Forests.

4. **Prediction Aggregation:**
   - For classification tasks, predictions from all trees are aggregated using majority voting. For regression tasks, the average prediction from all trees is computed.


![Structure-of-Extra-Trees-Kapoor-2020-Extra-Trees-constructs-the-set-of-decision-trees](https://github.com/user-attachments/assets/5f22d70d-9975-4882-b36b-f79dc46258e1)

#### Key Terms

- **Extremely Randomized Trees (Extra Trees):** A variant of decision trees that introduces more randomness into the model by selecting both features and split values at random.

- **Bootstrap Sampling:** A technique used in Random Forests where samples are drawn with replacement to create different training datasets for each tree.

- **Feature Subset Selection:** The process of selecting a subset of features for building each tree, which helps introduce diversity among trees.

- **Variance Reduction:** The ability of an ensemble method to reduce variance compared to individual models, leading to improved generalization on unseen data.

#### Advantages of Extra Trees

- **Speed:** Extra Trees is generally faster than Random Forest due to its use of random splits rather than searching for optimal splits.

- **Robustness:** It performs well even with noisy features and can handle irrelevant features effectively due to its randomization approach.

- **Lower Variance:** Compared to traditional decision trees and even Random Forests, Extra Trees tends to have lower variance because of its highly randomized nature.

#### Disadvantages of Extra Trees

- **Bias Increase:** The random selection of split points can lead to increased bias in some scenarios; however, this can be mitigated by careful feature selection before modeling.

- **Less Interpretability:** While still interpretable compared to some other models, the randomness in splits makes it harder to understand individual tree behavior compared to standard decision trees or Random Forests.

#### Applications

- **Classification Tasks:** Suitable for various classification problems across domains such as finance (credit scoring), healthcare (disease prediction), and marketing (customer segmentation).

- **Regression Tasks:** Can also be applied in regression scenarios where predicting continuous outcomes is required.

- **Feature Selection:** Useful in scenarios where feature selection has been performed prior to modeling, as it can effectively handle irrelevant features.

#### Evaluation Metrics

To evaluate the performance of Extra Trees, several metrics can be used:

- **Accuracy:** The proportion of correctly predicted instances out of total instances.

- **Precision and Recall:** Important for imbalanced datasets; precision measures the correctness among positive predictions while recall measures how well positive instances are captured.

- **F1 Score:** The harmonic mean of precision and recall, providing a balance between them.

- **Area Under Curve (AUC):** Evaluates model performance across all classification thresholds; higher values indicate better performance.

- **Cross-Validation Scores:** Using techniques like k-fold cross-validation helps assess model stability and generalization ability across different subsets of data.

#### Conclusion

The Extra Trees Algorithm is a powerful ensemble method that combines the strengths of decision trees with enhanced randomness to improve performance while reducing computational cost. Its ability to handle noise and irrelevant features makes it a robust choice for various classification and regression tasks. Understanding its workings, advantages, disadvantages, and evaluation metrics allows practitioners to effectively implement it in machine learning projects.
